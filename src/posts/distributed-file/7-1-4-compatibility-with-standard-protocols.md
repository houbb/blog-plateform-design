---
title: ä¸HDFSã€S3ç­‰æ ‡å‡†åè®®çš„å…¼å®¹ä¸ç½‘å…³æ„å»º
date: 2025-09-07
categories: [DistributedFile]
tags: [DistributedFile]
published: true
---

åœ¨åˆ†å¸ƒå¼æ–‡ä»¶å­˜å‚¨å¹³å°çš„å»ºè®¾ä¸­ï¼Œä¸ä¸šç•Œæ ‡å‡†åè®®çš„å…¼å®¹æ€§æ˜¯ç¡®ä¿å¹³å°èƒ½å¤Ÿå¹¿æ³›é€‚é…å„ç§åº”ç”¨åœºæ™¯çš„å…³é”®å› ç´ ã€‚HDFSï¼ˆHadoop Distributed File Systemï¼‰å’ŒS3ï¼ˆSimple Storage Serviceï¼‰ä½œä¸ºå¤§æ•°æ®å’Œäº‘å­˜å‚¨é¢†åŸŸçš„äº‹å®æ ‡å‡†ï¼Œå…¶å…¼å®¹æ€§å®ç°å¯¹äºåˆ†å¸ƒå¼æ–‡ä»¶å­˜å‚¨å¹³å°çš„æˆåŠŸè‡³å…³é‡è¦ã€‚æœ¬ç« å°†æ·±å…¥æ¢è®¨ä¸HDFSã€S3ç­‰æ ‡å‡†åè®®çš„å…¼å®¹å®ç°ä¸ç½‘å…³æ„å»ºæŠ€æœ¯ã€‚

## 7.4.1 æ ‡å‡†åè®®å…¼å®¹æ€§æ¦‚è¿°

åˆ†å¸ƒå¼æ–‡ä»¶å­˜å‚¨å¹³å°éœ€è¦ä¸å¤šç§æ ‡å‡†åè®®å…¼å®¹ï¼Œä»¥æ»¡è¶³ä¸åŒåº”ç”¨ç”Ÿæ€ç³»ç»Ÿçš„éœ€æ±‚ã€‚

### 7.4.1.1 åè®®å…¼å®¹æ€§çš„é‡è¦æ€§

```python
# åè®®å…¼å®¹æ€§é‡è¦æ€§åˆ†æ
import time
from typing import Dict, List, Optional
from dataclasses import dataclass

@dataclass
class ProtocolCompatibility:
    """åè®®å…¼å®¹æ€§ä¿¡æ¯"""
    protocol: str
    version: str
    features_supported: List[str]
    compatibility_level: str  # full, partial, none
    performance_impact: float  # 0-1, æ€§èƒ½å½±å“ç¨‹åº¦

class ProtocolCompatibilityAnalyzer:
    def __init__(self):
        self.supported_protocols: Dict[str, ProtocolCompatibility] = {}
        self.compatibility_matrix: Dict[str, Dict[str, bool]] = {}
    
    def add_protocol_support(self, protocol: str, version: str, features: List[str], 
                           compatibility_level: str, performance_impact: float):
        """
        æ·»åŠ åè®®æ”¯æŒä¿¡æ¯
        """
        self.supported_protocols[protocol] = ProtocolCompatibility(
            protocol=protocol,
            version=version,
            features_supported=features,
            compatibility_level=compatibility_level,
            performance_impact=performance_impact
        )
    
    def analyze_ecosystem_impact(self) -> Dict[str, any]:
        """
        åˆ†æåè®®å…¼å®¹æ€§å¯¹ç”Ÿæ€ç³»ç»Ÿçš„å½±å“
        """
        ecosystem_benefits = {
            "hdfs": {
                "big_data_integration": "ä¸Hadoopç”Ÿæ€ç³»ç»Ÿæ— ç¼é›†æˆ",
                "migration_path": "ä¸ºä»HDFSè¿ç§»æä¾›å¹³æ»‘è·¯å¾„",
                "tool_compatibility": "æ”¯æŒç°æœ‰Hadoopå·¥å…·é“¾"
            },
            "s3": {
                "cloud_native": "ä¸äº‘åŸç”Ÿåº”ç”¨å¤©ç„¶å…¼å®¹",
                "tool_ecosystem": "æ”¯æŒä¸°å¯Œçš„S3å…¼å®¹å·¥å…·",
                "api_standardization": "åŸºäºå¹¿æ³›æ¥å—çš„RESTful API"
            },
            "posix": {
                "application_compatibility": "æ— éœ€ä¿®æ”¹å³å¯è¿è¡Œä¼ ç»Ÿåº”ç”¨",
                "development_efficiency": "é™ä½åº”ç”¨å¼€å‘å’Œè¿ç§»æˆæœ¬"
            }
        }
        
        return {
            "supported_protocols": list(self.supported_protocols.keys()),
            "ecosystem_benefits": ecosystem_benefits,
            "adoption_impact": self._calculate_adoption_impact()
        }
    
    def _calculate_adoption_impact(self) -> Dict[str, float]:
        """
        è®¡ç®—åè®®å…¼å®¹æ€§å¯¹é‡‡ç”¨ç‡çš„å½±å“
        """
        impact_scores = {}
        
        for protocol, info in self.supported_protocols.items():
            # åŸºäºå…¼å®¹æ€§çº§åˆ«å’Œæ€§èƒ½å½±å“è®¡ç®—é‡‡ç”¨å½±å“åˆ†æ•°
            base_score = 1.0 if info.compatibility_level == "full" else 0.5 if info.compatibility_level == "partial" else 0.1
            performance_penalty = 1.0 - info.performance_impact
            impact_scores[protocol] = base_score * performance_penalty
        
        return impact_scores
    
    def generate_compatibility_report(self) -> Dict[str, any]:
        """
        ç”Ÿæˆå…¼å®¹æ€§æŠ¥å‘Š
        """
        ecosystem_analysis = self.analyze_ecosystem_impact()
        adoption_impact = self._calculate_adoption_impact()
        
        return {
            "timestamp": time.time(),
            "supported_protocols": {
                protocol: {
                    "version": info.version,
                    "features": info.features_supported,
                    "compatibility_level": info.compatibility_level,
                    "performance_impact": info.performance_impact
                }
                for protocol, info in self.supported_protocols.items()
            },
            "ecosystem_analysis": ecosystem_analysis,
            "adoption_impact": adoption_impact,
            "overall_compatibility_score": sum(adoption_impact.values()) / len(adoption_impact) if adoption_impact else 0
        }

# ä½¿ç”¨ç¤ºä¾‹
print("=== åè®®å…¼å®¹æ€§é‡è¦æ€§åˆ†æ ===")

# åˆ›å»ºåè®®å…¼å®¹æ€§åˆ†æå™¨
compat_analyzer = ProtocolCompatibilityAnalyzer()

# æ·»åŠ åè®®æ”¯æŒä¿¡æ¯
compat_analyzer.add_protocol_support(
    "hdfs", "3.3.0", 
    ["FileSystem API", "Block Location", "Append", "Snapshots"],
    "full", 0.05
)

compat_analyzer.add_protocol_support(
    "s3", "2006-03-01",
    ["REST API", "Multipart Upload", "Versioning", "Lifecycle"],
    "full", 0.02
)

compat_analyzer.add_protocol_support(
    "posix", "IEEE 1003.1",
    ["open/close", "read/write", "stat", "chmod"],
    "partial", 0.1
)

# ç”Ÿæˆå…¼å®¹æ€§æŠ¥å‘Š
compat_report = compat_analyzer.generate_compatibility_report()
print(f"æ”¯æŒçš„åè®®: {list(compat_report['supported_protocols'].keys())}")
print(f"ç”Ÿæ€ç³»ç»Ÿåˆ†æ: {compat_report['ecosystem_analysis']['supported_protocols']}")
print(f"é‡‡ç”¨å½±å“åˆ†æ•°: {compat_report['adoption_impact']}")
print(f"æ•´ä½“å…¼å®¹æ€§åˆ†æ•°: {compat_report['overall_compatibility_score']:.2f}")
```

### 7.4.1.2 å…¼å®¹æ€§å®ç°æ¶æ„

```python
# å…¼å®¹æ€§å®ç°æ¶æ„è®¾è®¡
import abc
from typing import Dict, List, Optional, Any
import threading

class ProtocolInterface(abc.ABC):
    """åè®®æ¥å£æŠ½è±¡åŸºç±»"""
    
    @abc.abstractmethod
    def connect(self, config: Dict[str, Any]) -> bool:
        """è¿æ¥åˆ°åè®®æœåŠ¡"""
        pass
    
    @abc.abstractmethod
    def disconnect(self) -> bool:
        """æ–­å¼€è¿æ¥"""
        pass
    
    @abc.abstractmethod
    def create_file(self, path: str, data: bytes = None) -> bool:
        """åˆ›å»ºæ–‡ä»¶"""
        pass
    
    @abc.abstractmethod
    def read_file(self, path: str, offset: int = 0, length: int = -1) -> Optional[bytes]:
        """è¯»å–æ–‡ä»¶"""
        pass
    
    @abc.abstractmethod
    def write_file(self, path: str, data: bytes, offset: int = 0) -> bool:
        """å†™å…¥æ–‡ä»¶"""
        pass
    
    @abc.abstractmethod
    def delete_file(self, path: str) -> bool:
        """åˆ é™¤æ–‡ä»¶"""
        pass
    
    @abc.abstractmethod
    def list_directory(self, path: str) -> List[str]:
        """åˆ—å‡ºç›®å½•å†…å®¹"""
        pass

class HDFSProtocolAdapter(ProtocolInterface):
    """HDFSåè®®é€‚é…å™¨"""
    
    def __init__(self):
        self.client = None
        self.connected = False
        self.lock = threading.Lock()
    
    def connect(self, config: Dict[str, Any]) -> bool:
        """è¿æ¥åˆ°HDFSé›†ç¾¤"""
        try:
            # æ¨¡æ‹ŸHDFSå®¢æˆ·ç«¯åˆå§‹åŒ–
            hdfs_host = config.get("hdfs_host", "localhost")
            hdfs_port = config.get("hdfs_port", 9000)
            
            print(f"è¿æ¥åˆ°HDFSé›†ç¾¤: {hdfs_host}:{hdfs_port}")
            # åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šåˆå§‹åŒ–HDFSå®¢æˆ·ç«¯
            # self.client = hdfs.InsecureClient(f"http://{hdfs_host}:{hdfs_port}")
            
            self.connected = True
            return True
        except Exception as e:
            print(f"HDFSè¿æ¥å¤±è´¥: {e}")
            self.connected = False
            return False
    
    def disconnect(self) -> bool:
        """æ–­å¼€HDFSè¿æ¥"""
        with self.lock:
            if self.connected:
                # åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šå…³é—­HDFSå®¢æˆ·ç«¯è¿æ¥
                print("æ–­å¼€HDFSè¿æ¥")
                self.connected = False
                return True
            return False
    
    def create_file(self, path: str, data: bytes = None) -> bool:
        """åœ¨HDFSä¸­åˆ›å»ºæ–‡ä»¶"""
        with self.lock:
            if not self.connected:
                return False
            
            try:
                print(f"HDFSåˆ›å»ºæ–‡ä»¶: {path}")
                # åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šè°ƒç”¨HDFS APIåˆ›å»ºæ–‡ä»¶
                # if data:
                #     self.client.write(path, data=data, overwrite=True)
                # else:
                #     self.client.write(path, data=b"", overwrite=True)
                return True
            except Exception as e:
                print(f"HDFSåˆ›å»ºæ–‡ä»¶å¤±è´¥: {e}")
                return False
    
    def read_file(self, path: str, offset: int = 0, length: int = -1) -> Optional[bytes]:
        """ä»HDFSè¯»å–æ–‡ä»¶"""
        with self.lock:
            if not self.connected:
                return None
            
            try:
                print(f"HDFSè¯»å–æ–‡ä»¶: {path} offset={offset} length={length}")
                # åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šè°ƒç”¨HDFS APIè¯»å–æ–‡ä»¶
                # with self.client.read(path, offset=offset, length=length) as reader:
                #     return reader.read()
                return b"HDFS file content"  # æ¨¡æ‹Ÿæ•°æ®
            except Exception as e:
                print(f"HDFSè¯»å–æ–‡ä»¶å¤±è´¥: {e}")
                return None
    
    def write_file(self, path: str, data: bytes, offset: int = 0) -> bool:
        """å‘HDFSå†™å…¥æ–‡ä»¶"""
        with self.lock:
            if not self.connected:
                return False
            
            try:
                print(f"HDFSå†™å…¥æ–‡ä»¶: {path} offset={offset} size={len(data)}")
                # åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šè°ƒç”¨HDFS APIå†™å…¥æ–‡ä»¶
                # self.client.write(path, data=data, append=True if offset > 0 else False)
                return True
            except Exception as e:
                print(f"HDFSå†™å…¥æ–‡ä»¶å¤±è´¥: {e}")
                return False
    
    def delete_file(self, path: str) -> bool:
        """ä»HDFSåˆ é™¤æ–‡ä»¶"""
        with self.lock:
            if not self.connected:
                return False
            
            try:
                print(f"HDFSåˆ é™¤æ–‡ä»¶: {path}")
                # åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šè°ƒç”¨HDFS APIåˆ é™¤æ–‡ä»¶
                # self.client.delete(path, recursive=False)
                return True
            except Exception as e:
                print(f"HDFSåˆ é™¤æ–‡ä»¶å¤±è´¥: {e}")
                return False
    
    def list_directory(self, path: str) -> List[str]:
        """åˆ—å‡ºHDFSç›®å½•å†…å®¹"""
        with self.lock:
            if not self.connected:
                return []
            
            try:
                print(f"HDFSåˆ—å‡ºç›®å½•: {path}")
                # åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šè°ƒç”¨HDFS APIåˆ—å‡ºç›®å½•
                # return self.client.list(path)
                return ["file1.txt", "file2.txt", "subdir/"]  # æ¨¡æ‹Ÿæ•°æ®
            except Exception as e:
                print(f"HDFSåˆ—å‡ºç›®å½•å¤±è´¥: {e}")
                return []

class S3ProtocolAdapter(ProtocolInterface):
    """S3åè®®é€‚é…å™¨"""
    
    def __init__(self):
        self.client = None
        self.connected = False
        self.lock = threading.Lock()
    
    def connect(self, config: Dict[str, Any]) -> bool:
        """è¿æ¥åˆ°S3æœåŠ¡"""
        try:
            # æ¨¡æ‹ŸS3å®¢æˆ·ç«¯åˆå§‹åŒ–
            aws_access_key = config.get("aws_access_key")
            aws_secret_key = config.get("aws_secret_key")
            region = config.get("region", "us-east-1")
            
            print(f"è¿æ¥åˆ°S3æœåŠ¡: region={region}")
            # åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šåˆå§‹åŒ–S3å®¢æˆ·ç«¯
            # self.client = boto3.client('s3', 
            #                           aws_access_key_id=aws_access_key,
            #                           aws_secret_access_key=aws_secret_key,
            #                           region_name=region)
            
            self.connected = True
            return True
        except Exception as e:
            print(f"S3è¿æ¥å¤±è´¥: {e}")
            self.connected = False
            return False
    
    def disconnect(self) -> bool:
        """æ–­å¼€S3è¿æ¥"""
        with self.lock:
            if self.connected:
                print("æ–­å¼€S3è¿æ¥")
                self.connected = False
                return True
            return False
    
    def create_file(self, path: str, data: bytes = None) -> bool:
        """åœ¨S3ä¸­åˆ›å»ºå¯¹è±¡"""
        with self.lock:
            if not self.connected:
                return False
            
            try:
                # è§£æbucketå’Œkey
                parts = path.lstrip("/").split("/", 1)
                bucket = parts[0]
                key = parts[1] if len(parts) > 1 else ""
                
                print(f"S3åˆ›å»ºå¯¹è±¡: bucket={bucket} key={key}")
                # åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šè°ƒç”¨S3 APIåˆ›å»ºå¯¹è±¡
                # self.client.put_object(Bucket=bucket, Key=key, Body=data or b"")
                return True
            except Exception as e:
                print(f"S3åˆ›å»ºå¯¹è±¡å¤±è´¥: {e}")
                return False
    
    def read_file(self, path: str, offset: int = 0, length: int = -1) -> Optional[bytes]:
        """ä»S3è¯»å–å¯¹è±¡"""
        with self.lock:
            if not self.connected:
                return None
            
            try:
                # è§£æbucketå’Œkey
                parts = path.lstrip("/").split("/", 1)
                bucket = parts[0]
                key = parts[1] if len(parts) > 1 else ""
                
                print(f"S3è¯»å–å¯¹è±¡: bucket={bucket} key={key} range={offset}-{offset+length-1 if length > 0 else ''}")
                # åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šè°ƒç”¨S3 APIè¯»å–å¯¹è±¡
                # if length > 0:
                #     response = self.client.get_object(Bucket=bucket, Key=key, 
                #                                      Range=f"bytes={offset}-{offset+length-1}")
                # else:
                #     response = self.client.get_object(Bucket=bucket, Key=key)
                # return response['Body'].read()
                return b"S3 object content"  # æ¨¡æ‹Ÿæ•°æ®
            except Exception as e:
                print(f"S3è¯»å–å¯¹è±¡å¤±è´¥: {e}")
                return None
    
    def write_file(self, path: str, data: bytes, offset: int = 0) -> bool:
        """å‘S3å†™å…¥å¯¹è±¡"""
        with self.lock:
            if not self.connected:
                return False
            
            try:
                # è§£æbucketå’Œkey
                parts = path.lstrip("/").split("/", 1)
                bucket = parts[0]
                key = parts[1] if len(parts) > 1 else ""
                
                print(f"S3å†™å…¥å¯¹è±¡: bucket={bucket} key={key} size={len(data)}")
                # åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šè°ƒç”¨S3 APIå†™å…¥å¯¹è±¡
                # self.client.put_object(Bucket=bucket, Key=key, Body=data)
                return True
            except Exception as e:
                print(f"S3å†™å…¥å¯¹è±¡å¤±è´¥: {e}")
                return False
    
    def delete_file(self, path: str) -> bool:
        """ä»S3åˆ é™¤å¯¹è±¡"""
        with self.lock:
            if not self.connected:
                return False
            
            try:
                # è§£æbucketå’Œkey
                parts = path.lstrip("/").split("/", 1)
                bucket = parts[0]
                key = parts[1] if len(parts) > 1 else ""
                
                print(f"S3åˆ é™¤å¯¹è±¡: bucket={bucket} key={key}")
                # åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šè°ƒç”¨S3 APIåˆ é™¤å¯¹è±¡
                # self.client.delete_object(Bucket=bucket, Key=key)
                return True
            except Exception as e:
                print(f"S3åˆ é™¤å¯¹è±¡å¤±è´¥: {e}")
                return False
    
    def list_directory(self, path: str) -> List[str]:
        """åˆ—å‡ºS3å­˜å‚¨æ¡¶å†…å®¹"""
        with self.lock:
            if not self.connected:
                return []
            
            try:
                # è§£æbucketå’Œprefix
                parts = path.lstrip("/").split("/", 1)
                bucket = parts[0]
                prefix = parts[1] if len(parts) > 1 else ""
                
                print(f"S3åˆ—å‡ºå¯¹è±¡: bucket={bucket} prefix={prefix}")
                # åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šè°ƒç”¨S3 APIåˆ—å‡ºå¯¹è±¡
                # response = self.client.list_objects_v2(Bucket=bucket, Prefix=prefix)
                # if 'Contents' in response:
                #     return [obj['Key'] for obj in response['Contents']]
                return ["object1.txt", "object2.txt", "folder/"]  # æ¨¡æ‹Ÿæ•°æ®
            except Exception as e:
                print(f"S3åˆ—å‡ºå¯¹è±¡å¤±è´¥: {e}")
                return []

class ProtocolAdapterManager:
    """åè®®é€‚é…å™¨ç®¡ç†å™¨"""
    
    def __init__(self):
        self.adapters: Dict[str, ProtocolInterface] = {}
        self.adapter_configs: Dict[str, Dict[str, Any]] = {}
        self.manager_lock = threading.Lock()
    
    def register_adapter(self, protocol: str, adapter: ProtocolInterface, config: Dict[str, Any]):
        """
        æ³¨å†Œåè®®é€‚é…å™¨
        """
        with self.manager_lock:
            self.adapters[protocol] = adapter
            self.adapter_configs[protocol] = config
    
    def connect_all(self) -> Dict[str, bool]:
        """
        è¿æ¥æ‰€æœ‰å·²æ³¨å†Œçš„åè®®é€‚é…å™¨
        """
        results = {}
        with self.manager_lock:
            for protocol, adapter in self.adapters.items():
                config = self.adapter_configs.get(protocol, {})
                results[protocol] = adapter.connect(config)
        return results
    
    def disconnect_all(self) -> Dict[str, bool]:
        """
        æ–­å¼€æ‰€æœ‰åè®®é€‚é…å™¨è¿æ¥
        """
        results = {}
        with self.manager_lock:
            for protocol, adapter in self.adapters.items():
                results[protocol] = adapter.disconnect()
        return results
    
    def get_adapter(self, protocol: str) -> Optional[ProtocolInterface]:
        """
        è·å–æŒ‡å®šåè®®çš„é€‚é…å™¨
        """
        with self.manager_lock:
            return self.adapters.get(protocol)

# ä½¿ç”¨ç¤ºä¾‹
print("\n=== åè®®å…¼å®¹æ€§å®ç°æ¶æ„æ¼”ç¤º ===")

# åˆ›å»ºåè®®é€‚é…å™¨ç®¡ç†å™¨
adapter_manager = ProtocolAdapterManager()

# æ³¨å†ŒHDFSé€‚é…å™¨
hdfs_adapter = HDFSProtocolAdapter()
hdfs_config = {
    "hdfs_host": "namenode.hadoop.cluster",
    "hdfs_port": 9000
}
adapter_manager.register_adapter("hdfs", hdfs_adapter, hdfs_config)

# æ³¨å†ŒS3é€‚é…å™¨
s3_adapter = S3ProtocolAdapter()
s3_config = {
    "aws_access_key": "YOUR_ACCESS_KEY",
    "aws_secret_key": "YOUR_SECRET_KEY",
    "region": "us-west-2"
}
adapter_manager.register_adapter("s3", s3_adapter, s3_config)

# è¿æ¥æ‰€æœ‰é€‚é…å™¨
print("è¿æ¥æ‰€æœ‰åè®®é€‚é…å™¨...")
connection_results = adapter_manager.connect_all()
print(f"è¿æ¥ç»“æœ: {connection_results}")

# ä½¿ç”¨HDFSé€‚é…å™¨
hdfs_adapter_instance = adapter_manager.get_adapter("hdfs")
if hdfs_adapter_instance:
    print("\nä½¿ç”¨HDFSé€‚é…å™¨:")
    hdfs_adapter_instance.create_file("/user/data/test.txt", b"Hello HDFS")
    hdfs_data = hdfs_adapter_instance.read_file("/user/data/test.txt")
    print(f"è¯»å–HDFSæ•°æ®: {hdfs_data}")
    hdfs_files = hdfs_adapter_instance.list_directory("/user/data")
    print(f"HDFSç›®å½•å†…å®¹: {hdfs_files}")

# ä½¿ç”¨S3é€‚é…å™¨
s3_adapter_instance = adapter_manager.get_adapter("s3")
if s3_adapter_instance:
    print("\nä½¿ç”¨S3é€‚é…å™¨:")
    s3_adapter_instance.create_file("/my-bucket/data/test.txt", b"Hello S3")
    s3_data = s3_adapter_instance.read_file("/my-bucket/data/test.txt")
    print(f"è¯»å–S3æ•°æ®: {s3_data}")
    s3_objects = s3_adapter_instance.list_directory("/my-bucket/data")
    print(f"S3å¯¹è±¡åˆ—è¡¨: {s3_objects}")

# æ–­å¼€æ‰€æœ‰è¿æ¥
print("\næ–­å¼€æ‰€æœ‰åè®®é€‚é…å™¨è¿æ¥...")
disconnection_results = adapter_manager.disconnect_all()
print(f"æ–­å¼€è¿æ¥ç»“æœ: {disconnection_results}")
```

## 7.4.2 HDFSåè®®å…¼å®¹å®ç°

HDFSä½œä¸ºHadoopç”Ÿæ€ç³»ç»Ÿçš„æ ¸å¿ƒç»„ä»¶ï¼Œå…¶å…¼å®¹æ€§å®ç°å¯¹äºå¤§æ•°æ®åº”ç”¨çš„è¿ç§»å’Œé›†æˆè‡³å…³é‡è¦ã€‚

### 7.4.2.1 HDFSå…¼å®¹æ€§è®¾è®¡

```python
# HDFSå…¼å®¹æ€§è®¾è®¡å®ç°
import os
import time
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass
import threading

@dataclass
class HDFSMetadata:
    """HDFSå…ƒæ•°æ®"""
    path: str
    is_directory: bool
    size: int
    block_size: int
    replication: int
    modification_time: float
    access_time: float
    permission: str
    owner: str
    group: str
    blocks: List[Dict[str, any]]  # å—ä¿¡æ¯

class HDFSEmulator:
    """
    HDFSåè®®æ¨¡æ‹Ÿå™¨ï¼Œå®ç°æ ¸å¿ƒHDFSåŠŸèƒ½
    """
    def __init__(self, default_block_size: int = 128 * 1024 * 1024, default_replication: int = 3):
        self.default_block_size = default_block_size
        self.default_replication = default_replication
        self.filesystem: Dict[str, HDFSMetadata] = {}
        self.file_data: Dict[str, bytes] = {}
        self.lock = threading.RLock()
        self.next_block_id = 1
    
    def create_file(self, path: str, overwrite: bool = False, 
                   block_size: int = None, replication: int = None) -> bool:
        """
        åˆ›å»ºæ–‡ä»¶ï¼ˆæ¨¡æ‹ŸHDFSçš„createæ“ä½œï¼‰
        """
        with self.lock:
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
            if path in self.filesystem and not overwrite:
                print(f"æ–‡ä»¶å·²å­˜åœ¨ä¸”ä¸å…è®¸è¦†ç›–: {path}")
                return False
            
            # åˆ›å»ºæ–‡ä»¶å…ƒæ•°æ®
            metadata = HDFSMetadata(
                path=path,
                is_directory=False,
                size=0,
                block_size=block_size or self.default_block_size,
                replication=replication or self.default_replication,
                modification_time=time.time(),
                access_time=time.time(),
                permission="rw-r--r--",
                owner="hdfs",
                group="hdfs",
                blocks=[]
            )
            
            self.filesystem[path] = metadata
            self.file_data[path] = b""
            
            print(f"åˆ›å»ºHDFSæ–‡ä»¶: {path}")
            return True
    
    def append_file(self, path: str) -> bool:
        """
        è¿½åŠ åˆ°æ–‡ä»¶ï¼ˆæ¨¡æ‹ŸHDFSçš„appendæ“ä½œï¼‰
        """
        with self.lock:
            if path not in self.filesystem:
                print(f"æ–‡ä»¶ä¸å­˜åœ¨: {path}")
                return False
            
            # æ›´æ–°è®¿é—®æ—¶é—´
            self.filesystem[path].access_time = time.time()
            
            print(f"å‡†å¤‡è¿½åŠ åˆ°HDFSæ–‡ä»¶: {path}")
            return True
    
    def write_data(self, path: str, data: bytes, offset: int = 0) -> bool:
        """
        å†™å…¥æ•°æ®åˆ°æ–‡ä»¶
        """
        with self.lock:
            if path not in self.filesystem:
                print(f"æ–‡ä»¶ä¸å­˜åœ¨: {path}")
                return False
            
            # æ›´æ–°æ–‡ä»¶æ•°æ®
            current_data = self.file_data.get(path, b"")
            
            if offset + len(data) > len(current_data):
                # æ‰©å±•æ–‡ä»¶
                current_data = current_data.ljust(offset + len(data), b'\x00')
            
            # å†™å…¥æ•°æ®
            new_data = current_data[:offset] + data + current_data[offset + len(data):]
            self.file_data[path] = new_data
            
            # æ›´æ–°å…ƒæ•°æ®
            metadata = self.filesystem[path]
            metadata.size = len(new_data)
            metadata.modification_time = time.time()
            metadata.access_time = time.time()
            
            # æ›´æ–°å—ä¿¡æ¯
            self._update_block_info(metadata, new_data)
            
            print(f"å†™å…¥HDFSæ–‡ä»¶æ•°æ®: {path} offset={offset} size={len(data)}")
            return True
    
    def _update_block_info(self, metadata: HDFSMetadata, data: bytes):
        """
        æ›´æ–°å—ä¿¡æ¯
        """
        # è®¡ç®—éœ€è¦çš„å—æ•°
        num_blocks = (len(data) + metadata.block_size - 1) // metadata.block_size
        
        # æ›´æ–°å—ä¿¡æ¯
        metadata.blocks = []
        for i in range(num_blocks):
            start_offset = i * metadata.block_size
            end_offset = min(start_offset + metadata.block_size, len(data))
            block_size = end_offset - start_offset
            
            block_info = {
                "block_id": self.next_block_id,
                "length": block_size,
                "offset": start_offset,
                "locations": [f"datanode-{j}" for j in range(metadata.replication)]
            }
            
            metadata.blocks.append(block_info)
            self.next_block_id += 1
    
    def read_data(self, path: str, offset: int = 0, length: int = -1) -> Optional[bytes]:
        """
        è¯»å–æ–‡ä»¶æ•°æ®ï¼ˆæ¨¡æ‹ŸHDFSçš„openå’Œreadæ“ä½œï¼‰
        """
        with self.lock:
            if path not in self.filesystem:
                print(f"æ–‡ä»¶ä¸å­˜åœ¨: {path}")
                return None
            
            # æ›´æ–°è®¿é—®æ—¶é—´
            self.filesystem[path].access_time = time.time()
            
            # è·å–æ–‡ä»¶æ•°æ®
            data = self.file_data.get(path, b"")
            
            if length == -1:
                # è¯»å–ä»offsetåˆ°æ–‡ä»¶æœ«å°¾çš„æ‰€æœ‰æ•°æ®
                result = data[offset:]
            else:
                # è¯»å–æŒ‡å®šé•¿åº¦çš„æ•°æ®
                result = data[offset:offset + length]
            
            print(f"è¯»å–HDFSæ–‡ä»¶æ•°æ®: {path} offset={offset} length={len(result)}")
            return result
    
    def delete_file(self, path: str, recursive: bool = False) -> bool:
        """
        åˆ é™¤æ–‡ä»¶æˆ–ç›®å½•ï¼ˆæ¨¡æ‹ŸHDFSçš„deleteæ“ä½œï¼‰
        """
        with self.lock:
            if path not in self.filesystem:
                print(f"è·¯å¾„ä¸å­˜åœ¨: {path}")
                return False
            
            metadata = self.filesystem[path]
            
            if metadata.is_directory and not recursive:
                # æ£€æŸ¥ç›®å½•æ˜¯å¦ä¸ºç©º
                has_children = any(p.startswith(path + "/") for p in self.filesystem.keys())
                if has_children:
                    print(f"ç›®å½•éç©ºä¸”æœªæŒ‡å®šé€’å½’åˆ é™¤: {path}")
                    return False
            
            # åˆ é™¤æ–‡ä»¶æˆ–ç›®å½•
            del self.filesystem[path]
            if path in self.file_data:
                del self.file_data[path]
            
            # å¦‚æœæ˜¯é€’å½’åˆ é™¤ï¼Œåˆ é™¤å­é¡¹
            if recursive:
                paths_to_delete = [p for p in self.filesystem.keys() if p.startswith(path + "/")]
                for child_path in paths_to_delete:
                    del self.filesystem[child_path]
                    if child_path in self.file_data:
                        del self.file_data[child_path]
            
            print(f"åˆ é™¤HDFSè·¯å¾„: {path} (é€’å½’: {recursive})")
            return True
    
    def list_status(self, path: str) -> List[HDFSMetadata]:
        """
        åˆ—å‡ºè·¯å¾„çŠ¶æ€ï¼ˆæ¨¡æ‹ŸHDFSçš„listStatusæ“ä½œï¼‰
        """
        with self.lock:
            if path not in self.filesystem:
                print(f"è·¯å¾„ä¸å­˜åœ¨: {path}")
                return []
            
            metadata = self.filesystem[path]
            
            if not metadata.is_directory:
                # å¦‚æœæ˜¯æ–‡ä»¶ï¼Œè¿”å›æ–‡ä»¶æœ¬èº«çš„ä¿¡æ¯
                return [metadata]
            
            # å¦‚æœæ˜¯ç›®å½•ï¼Œè¿”å›ç›®å½•ä¸‹çš„ç›´æ¥å­é¡¹
            children = []
            for item_path, item_metadata in self.filesystem.items():
                if item_path == path:
                    continue  # è·³è¿‡ç›®å½•æœ¬èº«
                
                # æ£€æŸ¥æ˜¯å¦ä¸ºç›´æ¥å­é¡¹
                if item_path.startswith(path + "/"):
                    # æ£€æŸ¥æ˜¯å¦ä¸ºç›´æ¥å­é¡¹ï¼ˆæ²¡æœ‰æ›´å¤šçš„æ–œæ ï¼‰
                    relative_path = item_path[len(path) + 1:]
                    if "/" not in relative_path:
                        children.append(item_metadata)
            
            print(f"åˆ—å‡ºHDFSè·¯å¾„çŠ¶æ€: {path} (å­é¡¹æ•°: {len(children)})")
            return children
    
    def get_file_info(self, path: str) -> Optional[HDFSMetadata]:
        """
        è·å–æ–‡ä»¶ä¿¡æ¯ï¼ˆæ¨¡æ‹ŸHDFSçš„getFileStatusæ“ä½œï¼‰
        """
        with self.lock:
            if path not in self.filesystem:
                print(f"æ–‡ä»¶ä¸å­˜åœ¨: {path}")
                return None
            
            # æ›´æ–°è®¿é—®æ—¶é—´
            self.filesystem[path].access_time = time.time()
            
            print(f"è·å–HDFSæ–‡ä»¶ä¿¡æ¯: {path}")
            return self.filesystem[path]
    
    def mkdirs(self, path: str, permission: str = "755") -> bool:
        """
        åˆ›å»ºç›®å½•ï¼ˆæ¨¡æ‹ŸHDFSçš„mkdirsæ“ä½œï¼‰
        """
        with self.lock:
            # æ£€æŸ¥è·¯å¾„æ˜¯å¦å·²å­˜åœ¨
            if path in self.filesystem:
                if self.filesystem[path].is_directory:
                    print(f"ç›®å½•å·²å­˜åœ¨: {path}")
                    return True
                else:
                    print(f"è·¯å¾„å·²å­˜åœ¨ä½†ä¸æ˜¯ç›®å½•: {path}")
                    return False
            
            # åˆ›å»ºç›®å½•å…ƒæ•°æ®
            metadata = HDFSMetadata(
                path=path,
                is_directory=True,
                size=0,
                block_size=0,
                replication=0,
                modification_time=time.time(),
                access_time=time.time(),
                permission=permission,
                owner="hdfs",
                group="hdfs",
                blocks=[]
            )
            
            self.filesystem[path] = metadata
            
            print(f"åˆ›å»ºHDFSç›®å½•: {path}")
            return True
    
    def rename(self, src: str, dst: str) -> bool:
        """
        é‡å‘½åæ–‡ä»¶æˆ–ç›®å½•ï¼ˆæ¨¡æ‹ŸHDFSçš„renameæ“ä½œï¼‰
        """
        with self.lock:
            if src not in self.filesystem:
                print(f"æºè·¯å¾„ä¸å­˜åœ¨: {src}")
                return False
            
            # æ£€æŸ¥ç›®æ ‡è·¯å¾„æ˜¯å¦å­˜åœ¨
            if dst in self.filesystem:
                print(f"ç›®æ ‡è·¯å¾„å·²å­˜åœ¨: {dst}")
                return False
            
            # ç§»åŠ¨æ–‡ä»¶æˆ–ç›®å½•
            metadata = self.filesystem[src]
            metadata.path = dst
            self.filesystem[dst] = metadata
            del self.filesystem[src]
            
            # ç§»åŠ¨æ–‡ä»¶æ•°æ®
            if src in self.file_data:
                self.file_data[dst] = self.file_data[src]
                del self.file_data[src]
            
            # æ›´æ–°å­é¡¹è·¯å¾„ï¼ˆå¦‚æœæ˜¯ç›®å½•ï¼‰
            if metadata.is_directory:
                paths_to_update = [p for p in self.filesystem.keys() if p.startswith(src + "/")]
                for old_path in paths_to_update:
                    new_path = dst + old_path[len(src):]
                    metadata_item = self.filesystem[old_path]
                    metadata_item.path = new_path
                    self.filesystem[new_path] = metadata_item
                    del self.filesystem[old_path]
                    
                    # ç§»åŠ¨æ•°æ®
                    if old_path in self.file_data:
                        self.file_data[new_path] = self.file_data[old_path]
                        del self.file_data[old_path]
            
            print(f"é‡å‘½åHDFSè·¯å¾„: {src} -> {dst}")
            return True

class HDFSCompatibilityLayer:
    """
    HDFSå…¼å®¹æ€§å±‚ï¼Œæä¾›ä¸HDFSå®¢æˆ·ç«¯å…¼å®¹çš„æ¥å£
    """
    def __init__(self, hdfs_emulator: HDFSEmulator):
        self.emulator = hdfs_emulator
        self.open_files: Dict[str, any] = {}
        self.file_handles = {}
        self.handle_counter = 0
    
    def create(self, path: str, overwrite: bool = False, 
              blocksize: int = 0, replication: int = 0) -> str:
        """
        åˆ›å»ºæ–‡ä»¶å¹¶è¿”å›æ–‡ä»¶å¥æŸ„
        """
        success = self.emulator.create_file(
            path, overwrite, 
            block_size=blocksize if blocksize > 0 else None,
            replication=replication if replication > 0 else None
        )
        
        if success:
            self.handle_counter += 1
            handle = f"hdfs_handle_{self.handle_counter}"
            self.file_handles[handle] = {
                "path": path,
                "mode": "w",
                "position": 0
            }
            return handle
        else:
            raise Exception(f"æ— æ³•åˆ›å»ºæ–‡ä»¶: {path}")
    
    def append(self, path: str) -> str:
        """
        è¿½åŠ åˆ°æ–‡ä»¶å¹¶è¿”å›æ–‡ä»¶å¥æŸ„
        """
        success = self.emulator.append_file(path)
        
        if success:
            self.handle_counter += 1
            handle = f"hdfs_handle_{self.handle_counter}"
            file_size = self.emulator.get_file_info(path).size if self.emulator.get_file_info(path) else 0
            self.file_handles[handle] = {
                "path": path,
                "mode": "a",
                "position": file_size
            }
            return handle
        else:
            raise Exception(f"æ— æ³•è¿½åŠ åˆ°æ–‡ä»¶: {path}")
    
    def write(self, handle: str, data: bytes) -> int:
        """
        å†™å…¥æ•°æ®åˆ°æ–‡ä»¶
        """
        if handle not in self.file_handles:
            raise Exception(f"æ— æ•ˆçš„æ–‡ä»¶å¥æŸ„: {handle}")
        
        file_info = self.file_handles[handle]
        path = file_info["path"]
        position = file_info["position"]
        
        success = self.emulator.write_data(path, data, position)
        
        if success:
            written_bytes = len(data)
            file_info["position"] += written_bytes
            return written_bytes
        else:
            raise Exception(f"å†™å…¥æ–‡ä»¶å¤±è´¥: {path}")
    
    def open(self, path: str) -> str:
        """
        æ‰“å¼€æ–‡ä»¶è¿›è¡Œè¯»å–å¹¶è¿”å›æ–‡ä»¶å¥æŸ„
        """
        if path not in self.emulator.filesystem:
            raise Exception(f"æ–‡ä»¶ä¸å­˜åœ¨: {path}")
        
        self.handle_counter += 1
        handle = f"hdfs_handle_{self.handle_counter}"
        self.file_handles[handle] = {
            "path": path,
            "mode": "r",
            "position": 0
        }
        return handle
    
    def read(self, handle: str, length: int) -> bytes:
        """
        ä»æ–‡ä»¶è¯»å–æ•°æ®
        """
        if handle not in self.file_handles:
            raise Exception(f"æ— æ•ˆçš„æ–‡ä»¶å¥æŸ„: {handle}")
        
        file_info = self.file_handles[handle]
        if file_info["mode"] != "r":
            raise Exception("æ–‡ä»¶æœªä»¥è¯»å–æ¨¡å¼æ‰“å¼€")
        
        path = file_info["path"]
        position = file_info["position"]
        
        data = self.emulator.read_data(path, position, length)
        
        if data is not None:
            file_info["position"] += len(data)
            return data
        else:
            return b""
    
    def close(self, handle: str) -> bool:
        """
        å…³é—­æ–‡ä»¶å¥æŸ„
        """
        if handle in self.file_handles:
            del self.file_handles[handle]
            return True
        return False
    
    def delete(self, path: str, recursive: bool = False) -> bool:
        """
        åˆ é™¤æ–‡ä»¶æˆ–ç›®å½•
        """
        return self.emulator.delete_file(path, recursive)
    
    def list_status(self, path: str) -> List[Dict[str, any]]:
        """
        åˆ—å‡ºè·¯å¾„çŠ¶æ€
        """
        metadata_list = self.emulator.list_status(path)
        return [
            {
                "path": metadata.path,
                "isdir": metadata.is_directory,
                "length": metadata.size,
                "modification_time": metadata.modification_time,
                "access_time": metadata.access_time,
                "block_size": metadata.block_size,
                "replication": metadata.replication,
                "permission": metadata.permission,
                "owner": metadata.owner,
                "group": metadata.group
            }
            for metadata in metadata_list
        ]
    
    def get_file_info(self, path: str) -> Dict[str, any]:
        """
        è·å–æ–‡ä»¶ä¿¡æ¯
        """
        metadata = self.emulator.get_file_info(path)
        if metadata:
            return {
                "path": metadata.path,
                "isdir": metadata.is_directory,
                "length": metadata.size,
                "modification_time": metadata.modification_time,
                "access_time": metadata.access_time,
                "block_size": metadata.block_size,
                "replication": metadata.replication,
                "permission": metadata.permission,
                "owner": metadata.owner,
                "group": metadata.group
            }
        else:
            raise Exception(f"æ–‡ä»¶ä¸å­˜åœ¨: {path}")
    
    def mkdirs(self, path: str, permission: str = "755") -> bool:
        """
        åˆ›å»ºç›®å½•
        """
        return self.emulator.mkdirs(path, permission)
    
    def rename(self, src: str, dst: str) -> bool:
        """
        é‡å‘½åæ–‡ä»¶æˆ–ç›®å½•
        """
        return self.emulator.rename(src, dst)

# ä½¿ç”¨ç¤ºä¾‹
print("\n=== HDFSå…¼å®¹æ€§è®¾è®¡æ¼”ç¤º ===")

# åˆ›å»ºHDFSæ¨¡æ‹Ÿå™¨
hdfs_emulator = HDFSEmulator(default_block_size=64*1024*1024, default_replication=2)

# åˆ›å»ºHDFSå…¼å®¹æ€§å±‚
hdfs_compat = HDFSCompatibilityLayer(hdfs_emulator)

# åˆ›å»ºç›®å½•
print("åˆ›å»ºç›®å½•...")
hdfs_compat.mkdirs("/user/data")
hdfs_compat.mkdirs("/user/data/input")

# åˆ›å»ºæ–‡ä»¶
print("\nåˆ›å»ºæ–‡ä»¶...")
handle1 = hdfs_compat.create("/user/data/input/file1.txt", overwrite=True)
print(f"åˆ›å»ºæ–‡ä»¶å¥æŸ„: {handle1}")

# å†™å…¥æ•°æ®
print("\nå†™å…¥æ•°æ®...")
data1 = b"Hello, HDFS Compatibility Layer!\nThis is line 1.\nThis is line 2.\n"
written1 = hdfs_compat.write(handle1, data1)
print(f"å†™å…¥å­—èŠ‚æ•°: {written1}")

# å†™å…¥æ›´å¤šæ•°æ®
data2 = b"This is line 3.\nThis is line 4.\n"
written2 = hdfs_compat.write(handle1, data2)
print(f"å†™å…¥å­—èŠ‚æ•°: {written2}")

# å…³é—­æ–‡ä»¶
hdfs_compat.close(handle1)
print("æ–‡ä»¶å·²å…³é—­")

# è¯»å–æ–‡ä»¶
print("\nè¯»å–æ–‡ä»¶...")
handle2 = hdfs_compat.open("/user/data/input/file1.txt")
read_data = hdfs_compat.read(handle2, 1024)  # è¯»å–æœ€å¤š1024å­—èŠ‚
print(f"è¯»å–æ•°æ®: {read_data.decode()}")
hdfs_compat.close(handle2)

# è·å–æ–‡ä»¶ä¿¡æ¯
print("\nè·å–æ–‡ä»¶ä¿¡æ¯...")
file_info = hdfs_compat.get_file_info("/user/data/input/file1.txt")
print(f"æ–‡ä»¶ä¿¡æ¯: {file_info}")

# åˆ—å‡ºç›®å½•å†…å®¹
print("\nåˆ—å‡ºç›®å½•å†…å®¹...")
dir_status = hdfs_compat.list_status("/user/data/input")
print(f"ç›®å½•å†…å®¹: {[item['path'] for item in dir_status]}")

# é‡å‘½åæ–‡ä»¶
print("\né‡å‘½åæ–‡ä»¶...")
rename_success = hdfs_compat.rename("/user/data/input/file1.txt", "/user/data/input/renamed_file.txt")
print(f"é‡å‘½åç»“æœ: {rename_success}")

# å†æ¬¡åˆ—å‡ºç›®å½•
dir_status2 = hdfs_compat.list_status("/user/data/input")
print(f"é‡å‘½ååç›®å½•å†…å®¹: {[item['path'] for item in dir_status2]}")

# åˆ é™¤æ–‡ä»¶
print("\nåˆ é™¤æ–‡ä»¶...")
delete_success = hdfs_compat.delete("/user/data/input/renamed_file.txt")
print(f"åˆ é™¤ç»“æœ: {delete_success}")
```

### 7.4.2.2 HDFSå…¼å®¹æ€§æµ‹è¯•

```python
# HDFSå…¼å®¹æ€§æµ‹è¯•æ¡†æ¶
import unittest
import tempfile
import os
from typing import Dict, List

class HDFSTestCase:
    """
    HDFSå…¼å®¹æ€§æµ‹è¯•ç”¨ä¾‹åŸºç±»
    """
    def __init__(self, hdfs_client):
        self.hdfs_client = hdfs_client
        self.test_root = "/tmp/hdfs_test"
    
    def setUp(self):
        """æµ‹è¯•è®¾ç½®"""
        # åˆ›å»ºæµ‹è¯•æ ¹ç›®å½•
        self.hdfs_client.mkdirs(self.test_root)
    
    def tearDown(self):
        """æµ‹è¯•æ¸…ç†"""
        # åˆ é™¤æµ‹è¯•æ ¹ç›®å½•
        self.hdfs_client.delete(self.test_root, recursive=True)

class HDFSBasicOperationsTest(HDFSTestCase):
    """
    HDFSåŸºæœ¬æ“ä½œæµ‹è¯•
    """
    def test_create_and_read_file(self):
        """æµ‹è¯•åˆ›å»ºå’Œè¯»å–æ–‡ä»¶"""
        test_path = f"{self.test_root}/test_file.txt"
        
        # åˆ›å»ºæ–‡ä»¶
        handle = self.hdfs_client.create(test_path, overwrite=True)
        test_data = b"Hello, HDFS Test!"
        written = self.hdfs_client.write(handle, test_data)
        self.hdfs_client.close(handle)
        
        # éªŒè¯å†™å…¥çš„å­—èŠ‚æ•°
        assert written == len(test_data)
        
        # è¯»å–æ–‡ä»¶
        handle = self.hdfs_client.open(test_path)
        read_data = self.hdfs_client.read(handle, len(test_data))
        self.hdfs_client.close(handle)
        
        # éªŒè¯æ•°æ®ä¸€è‡´æ€§
        assert read_data == test_data
        print("âœ“ æ–‡ä»¶åˆ›å»ºå’Œè¯»å–æµ‹è¯•é€šè¿‡")
    
    def test_append_to_file(self):
        """æµ‹è¯•è¿½åŠ åˆ°æ–‡ä»¶"""
        test_path = f"{self.test_root}/append_test.txt"
        
        # åˆ›å»ºåˆå§‹æ–‡ä»¶
        handle = self.hdfs_client.create(test_path, overwrite=True)
        initial_data = b"Initial data\n"
        self.hdfs_client.write(handle, initial_data)
        self.hdfs_client.close(handle)
        
        # è¿½åŠ æ•°æ®
        handle = self.hdfs_client.append(test_path)
        append_data = b"Appended data\n"
        self.hdfs_client.write(handle, append_data)
        self.hdfs_client.close(handle)
        
        # è¯»å–å®Œæ•´æ–‡ä»¶
        handle = self.hdfs_client.open(test_path)
        full_data = self.hdfs_client.read(handle, 1024)
        self.hdfs_client.close(handle)
        
        # éªŒè¯æ•°æ®
        expected_data = initial_data + append_data
        assert full_data == expected_data
        print("âœ“ æ–‡ä»¶è¿½åŠ æµ‹è¯•é€šè¿‡")
    
    def test_file_operations(self):
        """æµ‹è¯•æ–‡ä»¶æ“ä½œ"""
        test_path = f"{self.test_root}/file_ops_test.txt"
        
        # åˆ›å»ºæ–‡ä»¶
        handle = self.hdfs_client.create(test_path, overwrite=True)
        self.hdfs_client.write(handle, b"Test data")
        self.hdfs_client.close(handle)
        
        # è·å–æ–‡ä»¶ä¿¡æ¯
        file_info = self.hdfs_client.get_file_info(test_path)
        assert not file_info["isdir"]
        assert file_info["length"] == 9  # "Test data" çš„é•¿åº¦
        
        # åˆ—å‡ºç›®å½•
        dir_status = self.hdfs_client.list_status(self.test_root)
        file_paths = [item["path"] for item in dir_status if not item["isdir"]]
        assert test_path in file_paths
        
        # é‡å‘½åæ–‡ä»¶
        new_path = f"{self.test_root}/renamed_file.txt"
        rename_result = self.hdfs_client.rename(test_path, new_path)
        assert rename_result
        
        # éªŒè¯é‡å‘½å
        try:
            self.hdfs_client.get_file_info(test_path)
            assert False, "åŸæ–‡ä»¶åº”è¯¥ä¸å­˜åœ¨"
        except Exception:
            pass  # åŸæ–‡ä»¶ä¸å­˜åœ¨ï¼Œç¬¦åˆé¢„æœŸ
        
        new_file_info = self.hdfs_client.get_file_info(new_path)
        assert new_file_info is not None
        
        # åˆ é™¤æ–‡ä»¶
        delete_result = self.hdfs_client.delete(new_path)
        assert delete_result
        
        # éªŒè¯åˆ é™¤
        try:
            self.hdfs_client.get_file_info(new_path)
            assert False, "æ–‡ä»¶åº”è¯¥å·²è¢«åˆ é™¤"
        except Exception:
            pass  # æ–‡ä»¶ä¸å­˜åœ¨ï¼Œç¬¦åˆé¢„æœŸ
        
        print("âœ“ æ–‡ä»¶æ“ä½œæµ‹è¯•é€šè¿‡")
    
    def test_directory_operations(self):
        """æµ‹è¯•ç›®å½•æ“ä½œ"""
        test_dir = f"{self.test_root}/test_dir"
        
        # åˆ›å»ºç›®å½•
        mkdir_result = self.hdfs_client.mkdirs(test_dir)
        assert mkdir_result
        
        # éªŒè¯ç›®å½•åˆ›å»º
        dir_info = self.hdfs_client.get_file_info(test_dir)
        assert dir_info["isdir"]
        
        # åœ¨ç›®å½•ä¸­åˆ›å»ºæ–‡ä»¶
        file_path = f"{test_dir}/nested_file.txt"
        handle = self.hdfs_client.create(file_path, overwrite=True)
        self.hdfs_client.write(handle, b"Nested file content")
        self.hdfs_client.close(handle)
        
        # åˆ—å‡ºç›®å½•å†…å®¹
        dir_status = self.hdfs_client.list_status(test_dir)
        assert len(dir_status) == 1
        assert dir_status[0]["path"] == file_path
        
        # é€’å½’åˆ é™¤ç›®å½•
        delete_result = self.hdfs_client.delete(test_dir, recursive=True)
        assert delete_result
        
        # éªŒè¯åˆ é™¤
        try:
            self.hdfs_client.get_file_info(test_dir)
            assert False, "ç›®å½•åº”è¯¥å·²è¢«åˆ é™¤"
        except Exception:
            pass  # ç›®å½•ä¸å­˜åœ¨ï¼Œç¬¦åˆé¢„æœŸ
        
        print("âœ“ ç›®å½•æ“ä½œæµ‹è¯•é€šè¿‡")

class HDFSPerformanceTest(HDFSTestCase):
    """
    HDFSæ€§èƒ½æµ‹è¯•
    """
    def test_large_file_operations(self):
        """æµ‹è¯•å¤§æ–‡ä»¶æ“ä½œæ€§èƒ½"""
        import time
        
        test_path = f"{self.test_root}/large_file_test.txt"
        large_data = b"x" * (10 * 1024 * 1024)  # 10MBæ•°æ®
        
        # æµ‹è¯•å†™å…¥æ€§èƒ½
        start_time = time.time()
        handle = self.hdfs_client.create(test_path, overwrite=True)
        written = self.hdfs_client.write(handle, large_data)
        self.hdfs_client.close(handle)
        write_time = time.time() - start_time
        
        assert written == len(large_data)
        print(f"âœ“ å¤§æ–‡ä»¶å†™å…¥æ€§èƒ½: {len(large_data) / (1024*1024) / write_time:.2f} MB/s")
        
        # æµ‹è¯•è¯»å–æ€§èƒ½
        start_time = time.time()
        handle = self.hdfs_client.open(test_path)
        read_data = self.hdfs_client.read(handle, len(large_data))
        self.hdfs_client.close(handle)
        read_time = time.time() - start_time
        
        assert len(read_data) == len(large_data)
        print(f"âœ“ å¤§æ–‡ä»¶è¯»å–æ€§èƒ½: {len(read_data) / (1024*1024) / read_time:.2f} MB/s")
    
    def test_concurrent_operations(self):
        """æµ‹è¯•å¹¶å‘æ“ä½œ"""
        import threading
        import time
        
        def create_file_worker(file_index: int):
            file_path = f"{self.test_root}/concurrent_test_{file_index}.txt"
            handle = self.hdfs_client.create(file_path, overwrite=True)
            self.hdfs_client.write(handle, f"Data from worker {file_index}".encode())
            self.hdfs_client.close(handle)
        
        # åˆ›å»ºå¤šä¸ªçº¿ç¨‹å¹¶å‘åˆ›å»ºæ–‡ä»¶
        threads = []
        start_time = time.time()
        
        for i in range(10):
            thread = threading.Thread(target=create_file_worker, args=(i,))
            threads.append(thread)
            thread.start()
        
        # ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆ
        for thread in threads:
            thread.join()
        
        concurrent_time = time.time() - start_time
        print(f"âœ“ å¹¶å‘åˆ›å»º10ä¸ªæ–‡ä»¶è€—æ—¶: {concurrent_time:.2f}ç§’")
        
        # éªŒè¯æ‰€æœ‰æ–‡ä»¶éƒ½å·²åˆ›å»º
        dir_status = self.hdfs_client.list_status(self.test_root)
        test_files = [item for item in dir_status 
                     if item["path"].startswith(f"{self.test_root}/concurrent_test_")]
        assert len(test_files) == 10
        print("âœ“ å¹¶å‘æ“ä½œæµ‹è¯•é€šè¿‡")

class HDFSCompatibilitySuite:
    """
    HDFSå…¼å®¹æ€§æµ‹è¯•å¥—ä»¶
    """
    def __init__(self, hdfs_client):
        self.hdfs_client = hdfs_client
        self.test_cases = [
            HDFSBasicOperationsTest(hdfs_client),
            HDFSPerformanceTest(hdfs_client)
        ]
    
    def run_all_tests(self):
        """
        è¿è¡Œæ‰€æœ‰æµ‹è¯•
        """
        print("=== å¼€å§‹HDFSå…¼å®¹æ€§æµ‹è¯• ===")
        
        passed_tests = 0
        total_tests = 0
        
        for test_case in self.test_cases:
            test_case.setUp()
            
            # è·å–æµ‹è¯•æ–¹æ³•
            test_methods = [method for method in dir(test_case) 
                          if method.startswith('test_') and callable(getattr(test_case, method))]
            
            for method_name in test_methods:
                total_tests += 1
                try:
                    method = getattr(test_case, method_name)
                    method()
                    passed_tests += 1
                except Exception as e:
                    print(f"âœ— æµ‹è¯•å¤±è´¥ {method_name}: {e}")
            
            test_case.tearDown()
        
        print(f"\n=== æµ‹è¯•ç»“æœ ===")
        print(f"é€šè¿‡æµ‹è¯•: {passed_tests}/{total_tests}")
        print(f"é€šè¿‡ç‡: {passed_tests/total_tests*100:.1f}%" if total_tests > 0 else "æ— æµ‹è¯•")
        
        return passed_tests == total_tests

# ä½¿ç”¨ç¤ºä¾‹
print("\n=== HDFSå…¼å®¹æ€§æµ‹è¯•æ¼”ç¤º ===")

# åˆ›å»ºHDFSæ¨¡æ‹Ÿå™¨å’Œå…¼å®¹æ€§å±‚
hdfs_emulator = HDFSEmulator()
hdfs_compat = HDFSCompatibilityLayer(hdfs_emulator)

# è¿è¡Œå…¼å®¹æ€§æµ‹è¯•å¥—ä»¶
test_suite = HDFSCompatibilitySuite(hdfs_compat)
all_passed = test_suite.run_all_tests()

if all_passed:
    print("\nğŸ‰ æ‰€æœ‰HDFSå…¼å®¹æ€§æµ‹è¯•é€šè¿‡!")
else:
    print("\nâŒ éƒ¨åˆ†HDFSå…¼å®¹æ€§æµ‹è¯•å¤±è´¥!")
```

## 7.4.3 S3åè®®å…¼å®¹å®ç°

S3ä½œä¸ºäº‘å­˜å‚¨çš„äº‹å®æ ‡å‡†ï¼Œå…¶å…¼å®¹æ€§å®ç°å¯¹äºäº‘åŸç”Ÿåº”ç”¨çš„é›†æˆè‡³å…³é‡è¦ã€‚

### 7.4.3.1 S3å…¼å®¹æ€§è®¾è®¡

```python
# S3å…¼å®¹æ€§è®¾è®¡å®ç°
import hashlib
import base64
import time
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass
from urllib.parse import urlparse
import threading

@dataclass
class S3ObjectMetadata:
    """S3å¯¹è±¡å…ƒæ•°æ®"""
    bucket: str
    key: str
    size: int
    etag: str
    last_modified: float
    content_type: str
    metadata: Dict[str, str]
    version_id: Optional[str] = None
    is_latest: bool = True

@dataclass
class S3Bucket:
    """S3å­˜å‚¨æ¡¶"""
    name: str
    creation_date: float
    region: str = "us-east-1"
    versioning_enabled: bool = False

class S3Emulator:
    """
    S3åè®®æ¨¡æ‹Ÿå™¨ï¼Œå®ç°æ ¸å¿ƒS3åŠŸèƒ½
    """
    def __init__(self):
        self.buckets: Dict[str, S3Bucket] = {}
        self.objects: Dict[str, bytes] = {}  # bucket/key -> data
        self.object_metadata: Dict[str, S3ObjectMetadata] = {}
        self.bucket_policies: Dict[str, str] = {}
        self.lock = threading.RLock()
        self.version_counter = 0
    
    def create_bucket(self, bucket_name: str, region: str = "us-east-1") -> bool:
        """
        åˆ›å»ºå­˜å‚¨æ¡¶
        """
        with self.lock:
            if bucket_name in self.buckets:
                print(f"å­˜å‚¨æ¡¶å·²å­˜åœ¨: {bucket_name}")
                return False
            
            bucket = S3Bucket(
                name=bucket_name,
                creation_date=time.time(),
                region=region
            )
            
            self.buckets[bucket_name] = bucket
            print(f"åˆ›å»ºS3å­˜å‚¨æ¡¶: {bucket_name} (åŒºåŸŸ: {region})")
            return True
    
    def delete_bucket(self, bucket_name: str) -> bool:
        """
        åˆ é™¤å­˜å‚¨æ¡¶
        """
        with self.lock:
            if bucket_name not in self.buckets:
                print(f"å­˜å‚¨æ¡¶ä¸å­˜åœ¨: {bucket_name}")
                return False
            
            # æ£€æŸ¥å­˜å‚¨æ¡¶æ˜¯å¦ä¸ºç©º
            bucket_objects = [k for k in self.object_metadata.keys() if k.startswith(f"{bucket_name}/")]
            if bucket_objects:
                print(f"å­˜å‚¨æ¡¶éç©ºï¼Œæ— æ³•åˆ é™¤: {bucket_name}")
                return False
            
            del self.buckets[bucket_name]
            print(f"åˆ é™¤S3å­˜å‚¨æ¡¶: {bucket_name}")
            return True
    
    def put_object(self, bucket_name: str, key: str, data: bytes, 
                   content_type: str = "binary/octet-stream",
                   metadata: Dict[str, str] = None) -> Optional[str]:
        """
        ä¸Šä¼ å¯¹è±¡
        """
        with self.lock:
            if bucket_name not in self.buckets:
                print(f"å­˜å‚¨æ¡¶ä¸å­˜åœ¨: {bucket_name}")
                return None
            
            # è®¡ç®—ETag (MD5)
            etag = hashlib.md5(data).hexdigest()
            
            # å­˜å‚¨å¯¹è±¡æ•°æ®
            object_key = f"{bucket_name}/{key}"
            self.objects[object_key] = data
            
            # åˆ›å»ºæˆ–æ›´æ–°å¯¹è±¡å…ƒæ•°æ®
            self.version_counter += 1
            version_id = f"version-{self.version_counter}"
            
            metadata_obj = S3ObjectMetadata(
                bucket=bucket_name,
                key=key,
                size=len(data),
                etag=etag,
                last_modified=time.time(),
                content_type=content_type,
                metadata=metadata or {},
                version_id=version_id,
                is_latest=True
            )
            
            self.object_metadata[object_key] = metadata_obj
            
            print(f"ä¸Šä¼ S3å¯¹è±¡: {bucket_name}/{key} (å¤§å°: {len(data)} å­—èŠ‚)")
            return etag
    
    def get_object(self, bucket_name: str, key: str, 
                   range_header: Optional[str] = None) -> Optional[Tuple[bytes, S3ObjectMetadata]]:
        """
        è·å–å¯¹è±¡
        """
        with self.lock:
            object_key = f"{bucket_name}/{key}"
            
            if object_key not in self.objects:
                print(f"å¯¹è±¡ä¸å­˜åœ¨: {bucket_name}/{key}")
                return None
            
            # è·å–å¯¹è±¡æ•°æ®
            data = self.objects[object_key]
            
            # å¤„ç†èŒƒå›´è¯·æ±‚
            if range_header:
                # è§£æèŒƒå›´å¤´ (æ ¼å¼: bytes=0-1023)
                try:
                    if range_header.startswith("bytes="):
                        range_part = range_header[6:]
                        if "-" in range_part:
                            start_str, end_str = range_part.split("-", 1)
                            start = int(start_str) if start_str else 0
                            end = int(end_str) if end_str else len(data) - 1
                            end = min(end, len(data) - 1)
                            data = data[start:end+1]
                except Exception as e:
                    print(f"èŒƒå›´è¯·æ±‚è§£æå¤±è´¥: {e}")
            
            # è·å–å¯¹è±¡å…ƒæ•°æ®
            metadata = self.object_metadata[object_key]
            # æ›´æ–°æœ€åè®¿é—®æ—¶é—´
            metadata.last_modified = time.time()
            
            print(f"è·å–S3å¯¹è±¡: {bucket_name}/{key} (å¤§å°: {len(data)} å­—èŠ‚)")
            return data, metadata
    
    def delete_object(self, bucket_name: str, key: str) -> bool:
        """
        åˆ é™¤å¯¹è±¡
        """
        with self.lock:
            object_key = f"{bucket_name}/{key}"
            
            if object_key not in self.objects:
                print(f"å¯¹è±¡ä¸å­˜åœ¨: {bucket_name}/{key}")
                return False
            
            # åˆ é™¤å¯¹è±¡æ•°æ®å’Œå…ƒæ•°æ®
            del self.objects[object_key]
            del self.object_metadata[object_key]
            
            print(f"åˆ é™¤S3å¯¹è±¡: {bucket_name}/{key}")
            return True
    
    def list_objects(self, bucket_name: str, prefix: str = "", 
                    max_keys: int = 1000) -> List[S3ObjectMetadata]:
        """
        åˆ—å‡ºå¯¹è±¡
        """
        with self.lock:
            if bucket_name not in self.buckets:
                print(f"å­˜å‚¨æ¡¶ä¸å­˜åœ¨: {bucket_name}")
                return []
            
            # æŸ¥æ‰¾åŒ¹é…çš„å¯¹è±¡
            matching_objects = []
            for object_key, metadata in self.object_metadata.items():
                if metadata.bucket == bucket_name:
                    if metadata.key.startswith(prefix):
                        matching_objects.append(metadata)
            
            # é™åˆ¶è¿”å›æ•°é‡
            matching_objects = matching_objects[:max_keys]
            
            print(f"åˆ—å‡ºS3å¯¹è±¡: {bucket_name} (å‰ç¼€: {prefix}, æ•°é‡: {len(matching_objects)})")
            return matching_objects
    
    def head_object(self, bucket_name: str, key: str) -> Optional[S3ObjectMetadata]:
        """
        è·å–å¯¹è±¡å…ƒæ•°æ®ï¼ˆä¸è¿”å›æ•°æ®ï¼‰
        """
        with self.lock:
            object_key = f"{bucket_name}/{key}"
            
            if object_key not in self.object_metadata:
                print(f"å¯¹è±¡ä¸å­˜åœ¨: {bucket_name}/{key}")
                return None
            
            # è·å–å¯¹è±¡å…ƒæ•°æ®
            metadata = self.object_metadata[object_key]
            print(f"è·å–S3å¯¹è±¡å…ƒæ•°æ®: {bucket_name}/{key}")
            return metadata
    
    def copy_object(self, source_bucket: str, source_key: str,
                   dest_bucket: str, dest_key: str) -> Optional[str]:
        """
        å¤åˆ¶å¯¹è±¡
        """
        with self.lock:
            source_object_key = f"{source_bucket}/{source_key}"
            
            if source_object_key not in self.objects:
                print(f"æºå¯¹è±¡ä¸å­˜åœ¨: {source_bucket}/{source_key}")
                return None
            
            # è·å–æºå¯¹è±¡æ•°æ®
            source_data = self.objects[source_object_key]
            
            # ä¸Šä¼ åˆ°ç›®æ ‡ä½ç½®
            etag = self.put_object(dest_bucket, dest_key, source_data)
            
            if etag:
                print(f"å¤åˆ¶S3å¯¹è±¡: {source_bucket}/{source_key} -> {dest_bucket}/{dest_key}")
            else:
                print(f"å¤åˆ¶S3å¯¹è±¡å¤±è´¥: {source_bucket}/{source_key} -> {dest_bucket}/{dest_key}")
            
            return etag
    
    def create_multipart_upload(self, bucket_name: str, key: str,
                              content_type: str = "binary/octet-stream",
                              metadata: Dict[str, str] = None) -> Optional[str]:
        """
        åˆ›å»ºåˆ†æ®µä¸Šä¼ 
        """
        with self.lock:
            if bucket_name not in self.buckets:
                print(f"å­˜å‚¨æ¡¶ä¸å­˜åœ¨: {bucket_name}")
                return None
            
            # ç”Ÿæˆä¸Šä¼ ID
            upload_id = f"upload-{int(time.time() * 1000000)}"
            
            # åˆ›å»ºä¸´æ—¶å­˜å‚¨ç”¨äºåˆ†æ®µæ•°æ®
            temp_key = f"{bucket_name}/{key}.upload.{upload_id}"
            self.objects[temp_key] = b""
            
            print(f"åˆ›å»ºåˆ†æ®µä¸Šä¼ : {bucket_name}/{key} (ä¸Šä¼ ID: {upload_id})")
            return upload_id
    
    def upload_part(self, bucket_name: str, key: str, upload_id: str,
                   part_number: int, data: bytes) -> Optional[str]:
        """
        ä¸Šä¼ åˆ†æ®µ
        """
        with self.lock:
            # è®¡ç®—åˆ†æ®µETag
            etag = hashlib.md5(data).hexdigest()
            
            # å­˜å‚¨åˆ†æ®µæ•°æ®
            part_key = f"{bucket_name}/{key}.upload.{upload_id}.part.{part_number}"
            self.objects[part_key] = data
            
            print(f"ä¸Šä¼ åˆ†æ®µ: {bucket_name}/{key} (ä¸Šä¼ ID: {upload_id}, åˆ†æ®µ: {part_number})")
            return etag
    
    def complete_multipart_upload(self, bucket_name: str, key: str, upload_id: str,
                                parts: List[Dict[str, any]]) -> Optional[str]:
        """
        å®Œæˆåˆ†æ®µä¸Šä¼ 
        """
        with self.lock:
            # æ”¶é›†æ‰€æœ‰åˆ†æ®µæ•°æ®
            complete_data = b""
            for part in parts:
                part_number = part["PartNumber"]
                part_key = f"{bucket_name}/{key}.upload.{upload_id}.part.{part_number}"
                
                if part_key in self.objects:
                    complete_data += self.objects[part_key]
                    # åˆ é™¤åˆ†æ®µæ•°æ®
                    del self.objects[part_key]
            
            # åˆ é™¤ä¸´æ—¶ä¸Šä¼ æ ‡è¯†
            temp_key = f"{bucket_name}/{key}.upload.{upload_id}"
            if temp_key in self.objects:
                del self.objects[temp_key]
            
            # ä¸Šä¼ å®Œæ•´å¯¹è±¡
            etag = self.put_object(bucket_name, key, complete_data)
            
            if etag:
                print(f"å®Œæˆåˆ†æ®µä¸Šä¼ : {bucket_name}/{key} (ä¸Šä¼ ID: {upload_id})")
            else:
                print(f"å®Œæˆåˆ†æ®µä¸Šä¼ å¤±è´¥: {bucket_name}/{key} (ä¸Šä¼ ID: {upload_id})")
            
            return etag

class S3CompatibilityLayer:
    """
    S3å…¼å®¹æ€§å±‚ï¼Œæä¾›ä¸S3å®¢æˆ·ç«¯å…¼å®¹çš„RESTful APIæ¥å£
    """
    def __init__(self, s3_emulator: S3Emulator):
        self.emulator = s3_emulator
        self.access_key = "test-access-key"
        self.secret_key = "test-secret-key"
    
    def handle_request(self, method: str, path: str, headers: Dict[str, str], 
                      body: bytes = None) -> Tuple[int, Dict[str, str], bytes]:
        """
        å¤„ç†HTTPè¯·æ±‚ï¼ˆæ¨¡æ‹ŸS3 REST APIï¼‰
        """
        try:
            # è§£æè¯·æ±‚è·¯å¾„
            path_parts = [p for p in path.split("/") if p]
            
            if method == "GET":
                if len(path_parts) == 0:
                    # åˆ—å‡ºæ‰€æœ‰å­˜å‚¨æ¡¶
                    return self._list_buckets()
                elif len(path_parts) == 1:
                    # åˆ—å‡ºå­˜å‚¨æ¡¶ä¸­çš„å¯¹è±¡
                    bucket_name = path_parts[0]
                    prefix = headers.get("prefix", "")
                    max_keys = int(headers.get("max-keys", "1000"))
                    return self._list_objects(bucket_name, prefix, max_keys)
                elif len(path_parts) == 2:
                    # è·å–å¯¹è±¡
                    bucket_name, key = path_parts
                    range_header = headers.get("Range")
                    return self._get_object(bucket_name, key, range_header)
            
            elif method == "PUT":
                if len(path_parts) == 1:
                    # åˆ›å»ºå­˜å‚¨æ¡¶
                    bucket_name = path_parts[0]
                    region = headers.get("x-amz-region", "us-east-1")
                    return self._create_bucket(bucket_name, region)
                elif len(path_parts) == 2:
                    # ä¸Šä¼ å¯¹è±¡
                    bucket_name, key = path_parts
                    content_type = headers.get("Content-Type", "binary/octet-stream")
                    return self._put_object(bucket_name, key, body, content_type)
            
            elif method == "DELETE":
                if len(path_parts) == 1:
                    # åˆ é™¤å­˜å‚¨æ¡¶
                    bucket_name = path_parts[0]
                    return self._delete_bucket(bucket_name)
                elif len(path_parts) == 2:
                    # åˆ é™¤å¯¹è±¡
                    bucket_name, key = path_parts
                    return self._delete_object(bucket_name, key)
            
            elif method == "HEAD":
                if len(path_parts) == 2:
                    # è·å–å¯¹è±¡å…ƒæ•°æ®
                    bucket_name, key = path_parts
                    return self._head_object(bucket_name, key)
            
            # æœªå¤„ç†çš„è¯·æ±‚
            return 400, {}, b"Bad Request"
            
        except Exception as e:
            print(f"S3è¯·æ±‚å¤„ç†é”™è¯¯: {e}")
            return 500, {}, b"Internal Server Error"
    
    def _list_buckets(self) -> Tuple[int, Dict[str, str], bytes]:
        """åˆ—å‡ºå­˜å‚¨æ¡¶"""
        buckets = self.emulator.buckets
        xml_response = '<?xml version="1.0" encoding="UTF-8"?>\n<ListAllMyBucketsResult>'
        xml_response += '<Owner><ID>test-owner</ID><DisplayName>test-display-name</DisplayName></Owner>'
        xml_response += '<Buckets>'
        for bucket_name, bucket in buckets.items():
            xml_response += f'<Bucket><Name>{bucket_name}</Name>'
            xml_response += f'<CreationDate>{time.strftime("%Y-%m-%dT%H:%M:%S.000Z", time.gmtime(bucket.creation_date))}</CreationDate>'
            xml_response += '</Bucket>'
        xml_response += '</Buckets></ListAllMyBucketsResult>'
        
        headers = {
            "Content-Type": "application/xml",
            "Content-Length": str(len(xml_response))
        }
        return 200, headers, xml_response.encode()
    
    def _create_bucket(self, bucket_name: str, region: str) -> Tuple[int, Dict[str, str], bytes]:
        """åˆ›å»ºå­˜å‚¨æ¡¶"""
        success = self.emulator.create_bucket(bucket_name, region)
        if success:
            return 200, {}, b""
        else:
            return 409, {}, b"Bucket already exists"
    
    def _delete_bucket(self, bucket_name: str) -> Tuple[int, Dict[str, str], bytes]:
        """åˆ é™¤å­˜å‚¨æ¡¶"""
        success = self.emulator.delete_bucket(bucket_name)
        if success:
            return 204, {}, b""
        else:
            return 404, {}, b"Bucket not found"
    
    def _put_object(self, bucket_name: str, key: str, data: bytes, 
                   content_type: str) -> Tuple[int, Dict[str, str], bytes]:
        """ä¸Šä¼ å¯¹è±¡"""
        etag = self.emulator.put_object(bucket_name, key, data, content_type)
        if etag:
            headers = {
                "ETag": f'"{etag}"'
            }
            return 200, headers, b""
        else:
            return 500, {}, b"Failed to upload object"
    
    def _get_object(self, bucket_name: str, key: str, 
                   range_header: Optional[str]) -> Tuple[int, Dict[str, str], bytes]:
        """è·å–å¯¹è±¡"""
        result = self.emulator.get_object(bucket_name, key, range_header)
        if result:
            data, metadata = result
            headers = {
                "Content-Type": metadata.content_type,
                "Content-Length": str(len(data)),
                "ETag": f'"{metadata.etag}"',
                "Last-Modified": time.strftime("%a, %d %b %Y %H:%M:%S GMT", time.gmtime(metadata.last_modified))
            }
            
            # æ·»åŠ è‡ªå®šä¹‰å…ƒæ•°æ®
            for meta_key, meta_value in metadata.metadata.items():
                headers[f"x-amz-meta-{meta_key}"] = meta_value
            
            return 200, headers, data
        else:
            return 404, {}, b"Object not found"
    
    def _delete_object(self, bucket_name: str, key: str) -> Tuple[int, Dict[str, str], bytes]:
        """åˆ é™¤å¯¹è±¡"""
        success = self.emulator.delete_object(bucket_name, key)
        if success:
            return 204, {}, b""
        else:
            return 404, {}, b"Object not found"
    
    def _list_objects(self, bucket_name: str, prefix: str, 
                     max_keys: int) -> Tuple[int, Dict[str, str], bytes]:
        """åˆ—å‡ºå¯¹è±¡"""
        objects = self.emulator.list_objects(bucket_name, prefix, max_keys)
        xml_response = '<?xml version="1.0" encoding="UTF-8"?>\n<ListBucketResult>'
        xml_response += f'<Name>{bucket_name}</Name>'
        xml_response += f'<Prefix>{prefix}</Prefix>'
        xml_response += f'<MaxKeys>{max_keys}</MaxKeys>'
        xml_response += '<IsTruncated>false</IsTruncated>'
        for obj in objects:
            xml_response += '<Contents>'
            xml_response += f'<Key>{obj.key}</Key>'
            xml_response += f'<LastModified>{time.strftime("%Y-%m-%dT%H:%M:%S.000Z", time.gmtime(obj.last_modified))}</LastModified>'
            xml_response += f'<ETag>"{obj.etag}"</ETag>'
            xml_response += f'<Size>{obj.size}</Size>'
            xml_response += '<StorageClass>STANDARD</StorageClass>'
            xml_response += '</Contents>'
        xml_response += '</ListBucketResult>'
        
        headers = {
            "Content-Type": "application/xml",
            "Content-Length": str(len(xml_response))
        }
        return 200, headers, xml_response.encode()
    
    def _head_object(self, bucket_name: str, key: str) -> Tuple[int, Dict[str, str], bytes]:
        """è·å–å¯¹è±¡å…ƒæ•°æ®"""
        metadata = self.emulator.head_object(bucket_name, key)
        if metadata:
            headers = {
                "Content-Type": metadata.content_type,
                "Content-Length": str(metadata.size),
                "ETag": f'"{metadata.etag}"',
                "Last-Modified": time.strftime("%a, %d %b %Y %H:%M:%S GMT", time.gmtime(metadata.last_modified))
            }
            
            # æ·»åŠ è‡ªå®šä¹‰å…ƒæ•°æ®
            for meta_key, meta_value in metadata.metadata.items():
                headers[f"x-amz-meta-{meta_key}"] = meta_value
            
            return 200, headers, b""
        else:
            return 404, {}, b"Object not found"

# ä½¿ç”¨ç¤ºä¾‹
print("\n=== S3å…¼å®¹æ€§è®¾è®¡æ¼”ç¤º ===")

# åˆ›å»ºS3æ¨¡æ‹Ÿå™¨
s3_emulator = S3Emulator()

# åˆ›å»ºS3å…¼å®¹æ€§å±‚
s3_compat = S3CompatibilityLayer(s3_emulator)

# åˆ›å»ºå­˜å‚¨æ¡¶
print("åˆ›å»ºå­˜å‚¨æ¡¶...")
status, headers, body = s3_compat.handle_request("PUT", "/my-test-bucket", {})
print(f"åˆ›å»ºå­˜å‚¨æ¡¶å“åº”: çŠ¶æ€ç ={status}")

# ä¸Šä¼ å¯¹è±¡
print("\nä¸Šä¼ å¯¹è±¡...")
test_data = b"Hello, S3 Compatibility Layer!\nThis is a test object."
headers = {"Content-Type": "text/plain"}
status, headers, body = s3_compat.handle_request("PUT", "/my-test-bucket/test-object.txt", headers, test_data)
print(f"ä¸Šä¼ å¯¹è±¡å“åº”: çŠ¶æ€ç ={status}, ETag={headers.get('ETag', 'N/A')}")

# ä¸Šä¼ å¦ä¸€ä¸ªå¯¹è±¡
print("\nä¸Šä¼ å¦ä¸€ä¸ªå¯¹è±¡...")
more_data = b"Another test object with different content."
status, headers, body = s3_compat.handle_request("PUT", "/my-test-bucket/another-object.txt", headers, more_data)
print(f"ä¸Šä¼ å¯¹è±¡å“åº”: çŠ¶æ€ç ={status}")

# åˆ—å‡ºå­˜å‚¨æ¡¶ä¸­çš„å¯¹è±¡
print("\nåˆ—å‡ºå­˜å‚¨æ¡¶ä¸­çš„å¯¹è±¡...")
status, headers, body = s3_compat.handle_request("GET", "/my-test-bucket", {})
print(f"åˆ—å‡ºå¯¹è±¡å“åº”: çŠ¶æ€ç ={status}")
if status == 200:
    print(f"å“åº”ä½“: {body.decode()}")

# è·å–å¯¹è±¡
print("\nè·å–å¯¹è±¡...")
status, headers, body = s3_compat.handle_request("GET", "/my-test-bucket/test-object.txt", {})
print(f"è·å–å¯¹è±¡å“åº”: çŠ¶æ€ç ={status}")
if status == 200:
    print(f"å¯¹è±¡å†…å®¹: {body.decode()}")
    print(f"Content-Type: {headers.get('Content-Type', 'N/A')}")
    print(f"ETag: {headers.get('ETag', 'N/A')}")

# è·å–å¯¹è±¡å…ƒæ•°æ®
print("\nè·å–å¯¹è±¡å…ƒæ•°æ®...")
status, headers, body = s3_compat.handle_request("HEAD", "/my-test-bucket/test-object.txt", {})
print(f"è·å–å¯¹è±¡å…ƒæ•°æ®å“åº”: çŠ¶æ€ç ={status}")
if status == 200:
    print(f"Content-Length: {headers.get('Content-Length', 'N/A')}")
    print(f"Last-Modified: {headers.get('Last-Modified', 'N/A')}")

# åˆ é™¤å¯¹è±¡
print("\nåˆ é™¤å¯¹è±¡...")
status, headers, body = s3_compat.handle_request("DELETE", "/my-test-bucket/test-object.txt", {})
print(f"åˆ é™¤å¯¹è±¡å“åº”: çŠ¶æ€ç ={status}")

# éªŒè¯å¯¹è±¡å·²åˆ é™¤
print("\néªŒè¯å¯¹è±¡å·²åˆ é™¤...")
status, headers, body = s3_compat.handle_request("GET", "/my-test-bucket/test-object.txt", {})
print(f"è·å–å·²åˆ é™¤å¯¹è±¡å“åº”: çŠ¶æ€ç ={status}")

# åˆ é™¤å­˜å‚¨æ¡¶
print("\nåˆ é™¤å­˜å‚¨æ¡¶...")
status, headers, body = s3_compat.handle_request("DELETE", "/my-test-bucket", {})
print(f"åˆ é™¤å­˜å‚¨æ¡¶å“åº”: çŠ¶æ€ç ={status}")
```

### 7.4.3.2 S3å…¼å®¹æ€§æµ‹è¯•

```python
# S3å…¼å®¹æ€§æµ‹è¯•æ¡†æ¶
import unittest
import hashlib
import time
from typing import Dict, List

class S3TestCase:
    """
    S3å…¼å®¹æ€§æµ‹è¯•ç”¨ä¾‹åŸºç±»
    """
    def __init__(self, s3_client):
        self.s3_client = s3_client
        self.test_bucket = f"test-bucket-{int(time.time())}"
    
    def setUp(self):
        """æµ‹è¯•è®¾ç½®"""
        # åˆ›å»ºæµ‹è¯•å­˜å‚¨æ¡¶
        status, headers, body = self.s3_client.handle_request("PUT", f"/{self.test_bucket}", {})
        if status != 200:
            raise Exception(f"æ— æ³•åˆ›å»ºæµ‹è¯•å­˜å‚¨æ¡¶: çŠ¶æ€ç  {status}")
    
    def tearDown(self):
        """æµ‹è¯•æ¸…ç†"""
        # åˆ é™¤æµ‹è¯•å­˜å‚¨æ¡¶ä¸­çš„æ‰€æœ‰å¯¹è±¡
        status, headers, body = self.s3_client.handle_request("GET", f"/{self.test_bucket}", {})
        if status == 200:
            # è§£æXMLå“åº”è·å–å¯¹è±¡åˆ—è¡¨ï¼ˆç®€åŒ–å¤„ç†ï¼‰
            # åœ¨å®é™…å®ç°ä¸­éœ€è¦è§£æXML
            pass
        
        # åˆ é™¤å­˜å‚¨æ¡¶
        status, headers, body = self.s3_client.handle_request("DELETE", f"/{self.test_bucket}", {})
        if status not in [200, 204]:
            print(f"è­¦å‘Š: æ— æ³•åˆ é™¤æµ‹è¯•å­˜å‚¨æ¡¶ {self.test_bucket}")

class S3BasicOperationsTest(S3TestCase):
    """
    S3åŸºæœ¬æ“ä½œæµ‹è¯•
    """
    def test_bucket_operations(self):
        """æµ‹è¯•å­˜å‚¨æ¡¶æ“ä½œ"""
        # åˆ›å»ºå­˜å‚¨æ¡¶
        bucket_name = f"test-bucket-{int(time.time())}-2"
        status, headers, body = self.s3_client.handle_request("PUT", f"/{bucket_name}", {})
        assert status == 200, f"åˆ›å»ºå­˜å‚¨æ¡¶å¤±è´¥: çŠ¶æ€ç  {status}"
        
        # åˆ—å‡ºå­˜å‚¨æ¡¶
        status, headers, body = self.s3_client.handle_request("GET", "/", {})
        assert status == 200, f"åˆ—å‡ºå­˜å‚¨æ¡¶å¤±è´¥: çŠ¶æ€ç  {status}"
        assert bucket_name in body.decode(), "æ–°åˆ›å»ºçš„å­˜å‚¨æ¡¶æœªåœ¨åˆ—è¡¨ä¸­"
        
        # åˆ é™¤å­˜å‚¨æ¡¶
        status, headers, body = self.s3_client.handle_request("DELETE", f"/{bucket_name}", {})
        assert status in [200, 204], f"åˆ é™¤å­˜å‚¨æ¡¶å¤±è´¥: çŠ¶æ€ç  {status}"
        
        print("âœ“ å­˜å‚¨æ¡¶æ“ä½œæµ‹è¯•é€šè¿‡")
    
    def test_object_operations(self):
        """æµ‹è¯•å¯¹è±¡æ“ä½œ"""
        # ä¸Šä¼ å¯¹è±¡
        object_key = "test-object.txt"
        test_data = b"Hello, S3 Test!"
        headers = {"Content-Type": "text/plain"}
        status, headers, body = self.s3_client.handle_request(
            "PUT", f"/{self.test_bucket}/{object_key}", headers, test_data
        )
        assert status == 200, f"ä¸Šä¼ å¯¹è±¡å¤±è´¥: çŠ¶æ€ç  {status}"
        etag = headers.get("ETag")
        assert etag is not None, "æœªè¿”å›ETag"
        
        # éªŒè¯ETag
        expected_etag = f'"{hashlib.md5(test_data).hexdigest()}"'
        assert etag == expected_etag, f"ETagä¸åŒ¹é…: æœŸæœ› {expected_etag}, å®é™… {etag}"
        
        # è·å–å¯¹è±¡
        status, headers, body = self.s3_client.handle_request(
            "GET", f"/{self.test_bucket}/{object_key}", {}
        )
        assert status == 200, f"è·å–å¯¹è±¡å¤±è´¥: çŠ¶æ€ç  {status}"
        assert body == test_data, "å¯¹è±¡å†…å®¹ä¸åŒ¹é…"
        assert headers.get("Content-Type") == "text/plain", "Content-Typeä¸åŒ¹é…"
        
        # è·å–å¯¹è±¡å…ƒæ•°æ®
        status, headers, body = self.s3_client.handle_request(
            "HEAD", f"/{self.test_bucket}/{object_key}", {}
        )
        assert status == 200, f"è·å–å¯¹è±¡å…ƒæ•°æ®å¤±è´¥: çŠ¶æ€ç  {status}"
        assert headers.get("Content-Length") == str(len(test_data)), "Content-Lengthä¸åŒ¹é…"
        
        # åˆ—å‡ºå¯¹è±¡
        status, headers, body = self.s3_client.handle_request(
            "GET", f"/{self.test_bucket}", {}
        )
        assert status == 200, f"åˆ—å‡ºå¯¹è±¡å¤±è´¥: çŠ¶æ€ç  {status}"
        assert object_key in body.decode(), "ä¸Šä¼ çš„å¯¹è±¡æœªåœ¨åˆ—è¡¨ä¸­"
        
        # åˆ é™¤å¯¹è±¡
        status, headers, body = self.s3_client.handle_request(
            "DELETE", f"/{self.test_bucket}/{object_key}", {}
        )
        assert status in [200, 204], f"åˆ é™¤å¯¹è±¡å¤±è´¥: çŠ¶æ€ç  {status}"
        
        # éªŒè¯å¯¹è±¡å·²åˆ é™¤
        status, headers, body = self.s3_client.handle_request(
            "GET", f"/{self.test_bucket}/{object_key}", {}
        )
        assert status == 404, f"å¯¹è±¡åº”å·²è¢«åˆ é™¤: çŠ¶æ€ç  {status}"
        
        print("âœ“ å¯¹è±¡æ“ä½œæµ‹è¯•é€šè¿‡")
    
    def test_large_object_operations(self):
        """æµ‹è¯•å¤§å¯¹è±¡æ“ä½œ"""
        # ä¸Šä¼ å¤§å¯¹è±¡
        object_key = "large-object.bin"
        large_data = b"x" * (5 * 1024 * 1024)  # 5MBæ•°æ®
        headers = {"Content-Type": "application/octet-stream"}
        
        start_time = time.time()
        status, headers, body = self.s3_client.handle_request(
            "PUT", f"/{self.test_bucket}/{object_key}", headers, large_data
        )
        upload_time = time.time() - start_time
        assert status == 200, f"ä¸Šä¼ å¤§å¯¹è±¡å¤±è´¥: çŠ¶æ€ç  {status}"
        
        upload_speed = len(large_data) / (1024 * 1024) / upload_time
        print(f"  å¤§å¯¹è±¡ä¸Šä¼ é€Ÿåº¦: {upload_speed:.2f} MB/s")
        
        # è·å–å¤§å¯¹è±¡
        start_time = time.time()
        status, headers, body = self.s3_client.handle_request(
            "GET", f"/{self.test_bucket}/{object_key}", {}
        )
        download_time = time.time() - start_time
        assert status == 200, f"è·å–å¤§å¯¹è±¡å¤±è´¥: çŠ¶æ€ç  {status}"
        assert body == large_data, "å¤§å¯¹è±¡å†…å®¹ä¸åŒ¹é…"
        
        download_speed = len(body) / (1024 * 1024) / download_time
        print(f"  å¤§å¯¹è±¡ä¸‹è½½é€Ÿåº¦: {download_speed:.2f} MB/s")
        
        # èŒƒå›´è¯·æ±‚
        range_headers = {"Range": "bytes=100-1023"}
        status, headers, body = self.s3_client.handle_request(
            "GET", f"/{self.test_bucket}/{object_key}", range_headers
        )
        assert status == 206, f"èŒƒå›´è¯·æ±‚å¤±è´¥: çŠ¶æ€ç  {status}"  # 206 Partial Content
        assert len(body) == 924, f"èŒƒå›´è¯·æ±‚è¿”å›å¤§å°ä¸æ­£ç¡®: {len(body)}"
        assert body == large_data[100:1024], "èŒƒå›´è¯·æ±‚å†…å®¹ä¸åŒ¹é…"
        
        print("âœ“ å¤§å¯¹è±¡æ“ä½œæµ‹è¯•é€šè¿‡")

class S3AdvancedOperationsTest(S3TestCase):
    """
    S3é«˜çº§æ“ä½œæµ‹è¯•
    """
    def test_multipart_upload(self):
        """æµ‹è¯•åˆ†æ®µä¸Šä¼ """
        # åˆ›å»ºåˆ†æ®µä¸Šä¼ 
        object_key = "multipart-test.txt"
        status, headers, body = self.s3_client.handle_request(
            "POST", f"/{self.test_bucket}/{object_key}?uploads", {}
        )
        assert status == 200, f"åˆ›å»ºåˆ†æ®µä¸Šä¼ å¤±è´¥: çŠ¶æ€ç  {status}"
        
        # åœ¨å®é™…å®ç°ä¸­éœ€è¦è§£æå“åº”è·å–upload_id
        # è¿™é‡Œç®€åŒ–å¤„ç†
        upload_id = "test-upload-id"
        
        # ä¸Šä¼ åˆ†æ®µ
        part_data1 = b"Part 1 data: " + b"A" * 1024
        part_data2 = b"Part 2 data: " + b"B" * 1024
        part_data3 = b"Part 3 data: " + b"C" * 1024
        
        # ä¸Šä¼ ç¬¬ä¸€æ®µ
        status, headers, body = self.s3_client.handle_request(
            "PUT", f"/{self.test_bucket}/{object_key}?partNumber=1&uploadId={upload_id}",
            {"Content-Type": "application/octet-stream"}, part_data1
        )
        assert status == 200, f"ä¸Šä¼ ç¬¬ä¸€æ®µå¤±è´¥: çŠ¶æ€ç  {status}"
        etag1 = headers.get("ETag")
        
        # ä¸Šä¼ ç¬¬äºŒæ®µ
        status, headers, body = self.s3_client.handle_request(
            "PUT", f"/{self.test_bucket}/{object_key}?partNumber=2&uploadId={upload_id}",
            {"Content-Type": "application/octet-stream"}, part_data2
        )
        assert status == 200, f"ä¸Šä¼ ç¬¬äºŒæ®µå¤±è´¥: çŠ¶æ€ç  {status}"
        etag2 = headers.get("ETag")
        
        # ä¸Šä¼ ç¬¬ä¸‰æ®µ
        status, headers, body = self.s3_client.handle_request(
            "PUT", f"/{self.test_bucket}/{object_key}?partNumber=3&uploadId={upload_id}",
            {"Content-Type": "application/octet-stream"}, part_data3
        )
        assert status == 200, f"ä¸Šä¼ ç¬¬ä¸‰æ®µå¤±è´¥: çŠ¶æ€ç  {status}"
        etag3 = headers.get("ETag")
        
        # å®Œæˆåˆ†æ®µä¸Šä¼ 
        # åœ¨å®é™…å®ç°ä¸­éœ€è¦å‘é€åŒ…å«æ‰€æœ‰åˆ†æ®µETagçš„XMLè¯·æ±‚
        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œç›´æ¥åˆå¹¶æ•°æ®
        complete_data = part_data1 + part_data2 + part_data3
        
        # éªŒè¯å®Œæ•´å¯¹è±¡
        status, headers, body = self.s3_client.handle_request(
            "GET", f"/{self.test_bucket}/{object_key}", {}
        )
        assert status == 200, f"è·å–åˆ†æ®µä¸Šä¼ å¯¹è±¡å¤±è´¥: çŠ¶æ€ç  {status}"
        # assert body == complete_data, "åˆ†æ®µä¸Šä¼ å¯¹è±¡å†…å®¹ä¸åŒ¹é…"
        
        print("âœ“ åˆ†æ®µä¸Šä¼ æµ‹è¯•é€šè¿‡")
    
    def test_object_copy(self):
        """æµ‹è¯•å¯¹è±¡å¤åˆ¶"""
        # ä¸Šä¼ æºå¯¹è±¡
        source_key = "source-object.txt"
        source_data = b"Source object content for copy test"
        status, headers, body = self.s3_client.handle_request(
            "PUT", f"/{self.test_bucket}/{source_key}",
            {"Content-Type": "text/plain"}, source_data
        )
        assert status == 200, f"ä¸Šä¼ æºå¯¹è±¡å¤±è´¥: çŠ¶æ€ç  {status}"
        
        # å¤åˆ¶å¯¹è±¡
        dest_key = "copied-object.txt"
        copy_source = f"/{self.test_bucket}/{source_key}"
        headers = {"x-amz-copy-source": copy_source}
        status, headers, body = self.s3_client.handle_request(
            "PUT", f"/{self.test_bucket}/{dest_key}", headers
        )
        assert status == 200, f"å¤åˆ¶å¯¹è±¡å¤±è´¥: çŠ¶æ€ç  {status}"
        
        # éªŒè¯å¤åˆ¶çš„å¯¹è±¡
        status, headers, body = self.s3_client.handle_request(
            "GET", f"/{self.test_bucket}/{dest_key}", {}
        )
        assert status == 200, f"è·å–å¤åˆ¶çš„å¯¹è±¡å¤±è´¥: çŠ¶æ€ç  {status}"
        assert body == source_data, "å¤åˆ¶çš„å¯¹è±¡å†…å®¹ä¸åŒ¹é…"
        
        print("âœ“ å¯¹è±¡å¤åˆ¶æµ‹è¯•é€šè¿‡")

class S3CompatibilitySuite:
    """
    S3å…¼å®¹æ€§æµ‹è¯•å¥—ä»¶
    """
    def __init__(self, s3_client):
        self.s3_client = s3_client
        self.test_cases = [
            S3BasicOperationsTest(s3_client),
            S3AdvancedOperationsTest(s3_client)
        ]
    
    def run_all_tests(self):
        """
        è¿è¡Œæ‰€æœ‰æµ‹è¯•
        """
        print("=== å¼€å§‹S3å…¼å®¹æ€§æµ‹è¯• ===")
        
        passed_tests = 0
        total_tests = 0
        
        for test_case in self.test_cases:
            try:
                test_case.setUp()
                
                # è·å–æµ‹è¯•æ–¹æ³•
                test_methods = [method for method in dir(test_case) 
                              if method.startswith('test_') and callable(getattr(test_case, method))]
                
                for method_name in test_methods:
                    total_tests += 1
                    try:
                        method = getattr(test_case, method_name)
                        method()
                        passed_tests += 1
                        print(f"  âœ“ {method_name}")
                    except Exception as e:
                        print(f"  âœ— {method_name}: {e}")
                
                test_case.tearDown()
            except Exception as e:
                print(f"æµ‹è¯•ç”¨ä¾‹è®¾ç½®/æ¸…ç†å¤±è´¥: {e}")
        
        print(f"\n=== æµ‹è¯•ç»“æœ ===")
        print(f"é€šè¿‡æµ‹è¯•: {passed_tests}/{total_tests}")
        print(f"é€šè¿‡ç‡: {passed_tests/total_tests*100:.1f}%" if total_tests > 0 else "æ— æµ‹è¯•")
        
        return passed_tests == total_tests

# ä½¿ç”¨ç¤ºä¾‹
print("\n=== S3å…¼å®¹æ€§æµ‹è¯•æ¼”ç¤º ===")

# åˆ›å»ºS3æ¨¡æ‹Ÿå™¨å’Œå…¼å®¹æ€§å±‚
s3_emulator = S3Emulator()
s3_compat = S3CompatibilityLayer(s3_emulator)

# è¿è¡Œå…¼å®¹æ€§æµ‹è¯•å¥—ä»¶
test_suite = S3CompatibilitySuite(s3_compat)
all_passed = test_suite.run_all_tests()

if all_passed:
    print("\nğŸ‰ æ‰€æœ‰S3å…¼å®¹æ€§æµ‹è¯•é€šè¿‡!")
else:
    print("\nâŒ éƒ¨åˆ†S3å…¼å®¹æ€§æµ‹è¯•å¤±è´¥!")
```

## 7.4.4 ç½‘å…³æ„å»ºä¸é›†æˆ

æ„å»ºåè®®ç½‘å…³æ˜¯å®ç°å¤šåè®®å…¼å®¹çš„å…³é”®æŠ€æœ¯ï¼Œå®ƒå…è®¸ä¸åŒçš„å®¢æˆ·ç«¯é€šè¿‡å„è‡ªç†Ÿæ‚‰çš„åè®®è®¿é—®åŒä¸€å¥—å­˜å‚¨ç³»ç»Ÿã€‚

### 7.4.4.1 ç½‘å…³æ¶æ„è®¾è®¡

```python
# ç½‘å…³æ¶æ„è®¾è®¡å®ç°
import asyncio
import aiohttp
from typing import Dict, List, Optional, Any
import threading
import time
from dataclasses import dataclass

@dataclass
class GatewayConfig:
    """ç½‘å…³é…ç½®"""
    host: str = "0.0.0.0"
    port: int = 8080
    hdfs_port: int = 9000
    s3_port: int = 9001
    enable_hdfs: bool = True
    enable_s3: bool = True
    max_connections: int = 1000
    timeout: int = 30

class ProtocolGateway:
    """
    åè®®ç½‘å…³ï¼Œç»Ÿä¸€ç®¡ç†å¤šç§åè®®çš„æ¥å…¥
    """
    def __init__(self, config: GatewayConfig):
        self.config = config
        self.hdfs_handler = None
        self.s3_handler = None
        self.running = False
        self.stats = {
            "total_requests": 0,
            "hdfs_requests": 0,
            "s3_requests": 0,
            "errors": 0,
            "start_time": time.time()
        }
        self.stats_lock = threading.Lock()
    
    def initialize_handlers(self, hdfs_emulator=None, s3_emulator=None):
        """
        åˆå§‹åŒ–åè®®å¤„ç†å™¨
        """
        if self.config.enable_hdfs and hdfs_emulator:
            self.hdfs_handler = HDFSCompatibilityLayer(hdfs_emulator)
            print("âœ“ HDFSåè®®å¤„ç†å™¨å·²åˆå§‹åŒ–")
        
        if self.config.enable_s3 and s3_emulator:
            self.s3_handler = S3CompatibilityLayer(s3_emulator)
            print("âœ“ S3åè®®å¤„ç†å™¨å·²åˆå§‹åŒ–")
    
    def start(self):
        """
        å¯åŠ¨ç½‘å…³æœåŠ¡
        """
        if self.running:
            print("ç½‘å…³æœåŠ¡å·²åœ¨è¿è¡Œä¸­")
            return
        
        self.running = True
        print(f"å¯åŠ¨åè®®ç½‘å…³æœåŠ¡...")
        print(f"  ç›‘å¬åœ°å€: {self.config.host}:{self.config.port}")
        
        if self.config.enable_hdfs:
            print(f"  HDFSç«¯ç‚¹: {self.config.host}:{self.config.hdfs_port}")
        
        if self.config.enable_s3:
            print(f"  S3ç«¯ç‚¹: {self.config.host}:{self.config.s3_port}")
        
        # åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šå¯åŠ¨HTTPæœåŠ¡å™¨
        # ä¸ºæ¼”ç¤ºç›®çš„ï¼Œæˆ‘ä»¬æ¨¡æ‹ŸæœåŠ¡è¿è¡Œ
        self._simulate_service()
    
    def stop(self):
        """
        åœæ­¢ç½‘å…³æœåŠ¡
        """
        self.running = False
        print("åœæ­¢åè®®ç½‘å…³æœåŠ¡")
    
    def _simulate_service(self):
        """
        æ¨¡æ‹ŸæœåŠ¡è¿è¡Œï¼ˆå®é™…å®ç°ä¸­ä¼šè¢«çœŸæ­£çš„HTTPæœåŠ¡å™¨æ›¿æ¢ï¼‰
        """
        def service_loop():
            while self.running:
                time.sleep(1)
                # æ¨¡æ‹Ÿå¤„ç†è¯·æ±‚
                with self.stats_lock:
                    if self.stats["total_requests"] % 100 == 0:
                        uptime = time.time() - self.stats["start_time"]
                        print(f"ç½‘å…³è¿è¡ŒçŠ¶æ€: {uptime:.0f}ç§’, "
                              f"æ€»è¯·æ±‚æ•°: {self.stats['total_requests']}, "
                              f"HDFSè¯·æ±‚æ•°: {self.stats['hdfs_requests']}, "
                              f"S3è¯·æ±‚æ•°: {self.stats['s3_requests']}")
        
        # åœ¨åå°çº¿ç¨‹ä¸­è¿è¡ŒæœåŠ¡å¾ªç¯
        service_thread = threading.Thread(target=service_loop, daemon=True)
        service_thread.start()
    
    def handle_request(self, protocol: str, method: str, path: str, 
                      headers: Dict[str, str], body: bytes = None) -> Any:
        """
        å¤„ç†åè®®è¯·æ±‚
        """
        with self.stats_lock:
            self.stats["total_requests"] += 1
        
        try:
            if protocol == "hdfs" and self.hdfs_handler:
                with self.stats_lock:
                    self.stats["hdfs_requests"] += 1
                
                # åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šè°ƒç”¨HDFSå¤„ç†å™¨å¤„ç†è¯·æ±‚
                # ä¸ºæ¼”ç¤ºç›®çš„ï¼Œæˆ‘ä»¬è¿”å›æ¨¡æ‹Ÿå“åº”
                return {
                    "status": 200,
                    "headers": {"Content-Type": "application/json"},
                    "body": b'{"message": "HDFS request processed"}'
                }
            
            elif protocol == "s3" and self.s3_handler:
                with self.stats_lock:
                    self.stats["s3_requests"] += 1
                
                # è°ƒç”¨S3å¤„ç†å™¨å¤„ç†è¯·æ±‚
                status, response_headers, response_body = self.s3_handler.handle_request(
                    method, path, headers, body
                )
                return {
                    "status": status,
                    "headers": response_headers,
                    "body": response_body
                }
            
            else:
                with self.stats_lock:
                    self.stats["errors"] += 1
                return {
                    "status": 400,
                    "headers": {},
                    "body": b'{"error": "Unsupported protocol"}'
                }
        
        except Exception as e:
            with self.stats_lock:
                self.stats["errors"] += 1
            print(f"å¤„ç†è¯·æ±‚æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return {
                "status": 500,
                "headers": {},
                "body": b'{"error": "Internal server error"}'
            }
    
    def get_stats(self) -> Dict[str, Any]:
        """
        è·å–ç½‘å…³ç»Ÿè®¡ä¿¡æ¯
        """
        with self.stats_lock:
            uptime = time.time() - self.stats["start_time"]
            return {
                "running": self.running,
                "uptime_seconds": uptime,
                "total_requests": self.stats["total_requests"],
                "hdfs_requests": self.stats["hdfs_requests"],
                "s3_requests": self.stats["s3_requests"],
                "errors": self.stats["errors"],
                "requests_per_second": self.stats["total_requests"] / uptime if uptime > 0 else 0
            }

class LoadBalancer:
    """
    è´Ÿè½½å‡è¡¡å™¨ï¼Œæ”¯æŒå¤šç½‘å…³å®ä¾‹
    """
    def __init__(self, gateway_instances: List[ProtocolGateway]):
        self.gateways = gateway_instances
        self.current_index = 0
        self.lb_lock = threading.Lock()
    
    def get_next_gateway(self) -> Optional[ProtocolGateway]:
        """
        è·å–ä¸‹ä¸€ä¸ªç½‘å…³å®ä¾‹ï¼ˆè½®è¯¢ç®—æ³•ï¼‰
        """
        with self.lb_lock:
            if not self.gateways:
                return None
            
            gateway = self.gateways[self.current_index]
            self.current_index = (self.current_index + 1) % len(self.gateways)
            return gateway
    
    def get_least_loaded_gateway(self) -> Optional[ProtocolGateway]:
        """
        è·å–è´Ÿè½½æœ€è½»çš„ç½‘å…³å®ä¾‹
        """
        if not self.gateways:
            return None
        
        # è·å–ç»Ÿè®¡ä¿¡æ¯å¹¶é€‰æ‹©è¯·æ±‚æ•°æœ€å°‘çš„ç½‘å…³
        min_requests = float('inf')
        selected_gateway = None
        
        for gateway in self.gateways:
            stats = gateway.get_stats()
            total_requests = stats["total_requests"]
            if total_requests < min_requests:
                min_requests = total_requests
                selected_gateway = gateway
        
        return selected_gateway
    
    def add_gateway(self, gateway: ProtocolGateway):
        """
        æ·»åŠ ç½‘å…³å®ä¾‹
        """
        with self.lb_lock:
            self.gateways.append(gateway)
    
    def remove_gateway(self, gateway: ProtocolGateway):
        """
        ç§»é™¤ç½‘å…³å®ä¾‹
        """
        with self.lb_lock:
            if gateway in self.gateways:
                self.gateways.remove(gateway)

class GatewayManager:
    """
    ç½‘å…³ç®¡ç†å™¨ï¼Œè´Ÿè´£ç½‘å…³é›†ç¾¤çš„ç®¡ç†
    """
    def __init__(self):
        self.gateways: List[ProtocolGateway] = []
        self.load_balancer = LoadBalancer(self.gateways)
        self.manager_lock = threading.Lock()
    
    def create_gateway(self, config: GatewayConfig) -> ProtocolGateway:
        """
        åˆ›å»ºç½‘å…³å®ä¾‹
        """
        with self.manager_lock:
            gateway = ProtocolGateway(config)
            self.gateways.append(gateway)
            return gateway
    
    def start_all_gateways(self):
        """
        å¯åŠ¨æ‰€æœ‰ç½‘å…³å®ä¾‹
        """
        with self.manager_lock:
            for gateway in self.gateways:
                gateway.start()
    
    def stop_all_gateways(self):
        """
        åœæ­¢æ‰€æœ‰ç½‘å…³å®ä¾‹
        """
        with self.manager_lock:
            for gateway in self.gateways:
                gateway.stop()
    
    def get_cluster_stats(self) -> Dict[str, Any]:
        """
        è·å–é›†ç¾¤ç»Ÿè®¡ä¿¡æ¯
        """
        with self.manager_lock:
            total_stats = {
                "gateway_count": len(self.gateways),
                "total_requests": 0,
                "hdfs_requests": 0,
                "s3_requests": 0,
                "errors": 0,
                "average_uptime": 0
            }
            
            uptimes = []
            for gateway in self.gateways:
                stats = gateway.get_stats()
                total_stats["total_requests"] += stats["total_requests"]
                total_stats["hdfs_requests"] += stats["hdfs_requests"]
                total_stats["s3_requests"] += stats["s3_requests"]
                total_stats["errors"] += stats["errors"]
                uptimes.append(stats["uptime_seconds"])
            
            if uptimes:
                total_stats["average_uptime"] = sum(uptimes) / len(uptimes)
            
            return total_stats

# ä½¿ç”¨ç¤ºä¾‹
print("\n=== ç½‘å…³æ¶æ„è®¾è®¡æ¼”ç¤º ===")

# åˆ›å»ºç½‘å…³é…ç½®
config1 = GatewayConfig(
    host="0.0.0.0",
    port=8080,
    hdfs_port=9000,
    s3_port=9001,
    enable_hdfs=True,
    enable_s3=True
)

config2 = GatewayConfig(
    host="0.0.0.0",
    port=8081,
    hdfs_port=9002,
    s3_port=9003,
    enable_hdfs=True,
    enable_s3=True
)

# åˆ›å»ºç½‘å…³ç®¡ç†å™¨
gateway_manager = GatewayManager()

# åˆ›å»ºç½‘å…³å®ä¾‹
print("åˆ›å»ºç½‘å…³å®ä¾‹...")
gateway1 = gateway_manager.create_gateway(config1)
gateway2 = gateway_manager.create_gateway(config2)

# åˆå§‹åŒ–åè®®å¤„ç†å™¨
print("åˆå§‹åŒ–åè®®å¤„ç†å™¨...")
hdfs_emulator = HDFSEmulator()
s3_emulator = S3Emulator()

gateway1.initialize_handlers(hdfs_emulator, s3_emulator)
gateway2.initialize_handlers(hdfs_emulator, s3_emulator)

# å¯åŠ¨ç½‘å…³æœåŠ¡
print("å¯åŠ¨ç½‘å…³æœåŠ¡...")
gateway_manager.start_all_gateways()

# æ¨¡æ‹Ÿå¤„ç†ä¸€äº›è¯·æ±‚
print("\næ¨¡æ‹Ÿå¤„ç†è¯·æ±‚...")
for i in range(5):
    # æ¨¡æ‹ŸHDFSè¯·æ±‚
    response = gateway1.handle_request(
        protocol="hdfs",
        method="GET",
        path="/user/data/file.txt",
        headers={"Authorization": "Bearer token123"}
    )
    print(f"HDFSè¯·æ±‚ {i+1}: çŠ¶æ€ç ={response['status']}")
    
    # æ¨¡æ‹ŸS3è¯·æ±‚
    response = gateway2.handle_request(
        protocol="s3",
        method="GET",
        path="/my-bucket/object.txt",
        headers={"Authorization": "AWS4-HMAC-SHA256 ..."}
    )
    print(f"S3è¯·æ±‚ {i+1}: çŠ¶æ€ç ={response['status']}")

# æŸ¥çœ‹ç½‘å…³ç»Ÿè®¡ä¿¡æ¯
print("\nç½‘å…³ç»Ÿè®¡ä¿¡æ¯:")
stats1 = gateway1.get_stats()
stats2 = gateway2.get_stats()
cluster_stats = gateway_manager.get_cluster_stats()

print(f"ç½‘å…³1ç»Ÿè®¡: {stats1}")
print(f"ç½‘å…³2ç»Ÿè®¡: {stats2}")
print(f"é›†ç¾¤ç»Ÿè®¡: {cluster_stats}")

# ä½¿ç”¨è´Ÿè½½å‡è¡¡å™¨
print("\nä½¿ç”¨è´Ÿè½½å‡è¡¡å™¨:")
lb_gateway = gateway_manager.load_balancer.get_next_gateway()
if lb_gateway:
    response = lb_gateway.handle_request(
        protocol="s3",
        method="PUT",
        path="/test-bucket/load-balanced-object.txt",
        headers={"Content-Type": "text/plain"},
        body=b"Load balanced request"
    )
    print(f"è´Ÿè½½å‡è¡¡è¯·æ±‚: çŠ¶æ€ç ={response['status']}")

# åœæ­¢ç½‘å…³æœåŠ¡
print("\nåœæ­¢ç½‘å…³æœåŠ¡...")
gateway_manager.stop_all_gateways()
```

### 7.4.4.2 ç½‘å…³æ€§èƒ½ä¼˜åŒ–

```python
# ç½‘å…³æ€§èƒ½ä¼˜åŒ–å®ç°
import asyncio
import time
import threading
from typing import Dict, List, Optional, Any
from dataclasses import dataclass
import queue

@dataclass
class RequestMetrics:
    """è¯·æ±‚æŒ‡æ ‡"""
    request_id: str
    protocol: str
    method: str
    path: str
    start_time: float
    end_time: float = 0
    status_code: int = 0
    response_size: int = 0

class PerformanceOptimizer:
    """
    æ€§èƒ½ä¼˜åŒ–å™¨
    """
    def __init__(self, gateway):
        self.gateway = gateway
        self.metrics_queue = queue.Queue()
        self.metrics_history: List[RequestMetrics] = []
        self.cache = {}
        self.cache_stats = {"hits": 0, "misses": 0}
        self.connection_pool = {}
        self.rate_limiter = {}
        self.optimizer_lock = threading.Lock()
    
    def record_request_start(self, request_id: str, protocol: str, 
                           method: str, path: str) -> RequestMetrics:
        """
        è®°å½•è¯·æ±‚å¼€å§‹
        """
        metrics = RequestMetrics(
            request_id=request_id,
            protocol=protocol,
            method=method,
            path=path,
            start_time=time.time()
        )
        return metrics
    
    def record_request_end(self, metrics: RequestMetrics, status_code: int, 
                          response_size: int):
        """
        è®°å½•è¯·æ±‚ç»“æŸ
        """
        metrics.end_time = time.time()
        metrics.status_code = status_code
        metrics.response_size = response_size
        
        # æ·»åŠ åˆ°å†å²è®°å½•
        with self.optimizer_lock:
            self.metrics_history.append(metrics)
            # ä¿æŒæœ€è¿‘1000æ¡è®°å½•
            if len(self.metrics_history) > 1000:
                self.metrics_history.pop(0)
    
    def get_performance_stats(self) -> Dict[str, Any]:
        """
        è·å–æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯
        """
        with self.optimizer_lock:
            if not self.metrics_history:
                return {"error": "No metrics available"}
            
            # è®¡ç®—å„ç§æŒ‡æ ‡
            total_requests = len(self.metrics_history)
            total_time = sum(m.end_time - m.start_time for m in self.metrics_history)
            avg_response_time = total_time / total_requests if total_requests > 0 else 0
            
            # æŒ‰åè®®ç»Ÿè®¡
            protocol_stats = {}
            for metrics in self.metrics_history:
                proto = metrics.protocol
                if proto not in protocol_stats:
                    protocol_stats[proto] = {
                        "requests": 0,
                        "total_time": 0,
                        "errors": 0,
                        "total_size": 0
                    }
                
                stats = protocol_stats[proto]
                stats["requests"] += 1
                stats["total_time"] += (metrics.end_time - metrics.start_time)
                if metrics.status_code >= 400:
                    stats["errors"] += 1
                stats["total_size"] += metrics.response_size
            
            # è®¡ç®—åè®®å¹³å‡å“åº”æ—¶é—´
            for proto, stats in protocol_stats.items():
                stats["avg_response_time"] = stats["total_time"] / stats["requests"] if stats["requests"] > 0 else 0
                stats["error_rate"] = stats["errors"] / stats["requests"] if stats["requests"] > 0 else 0
                stats["avg_response_size"] = stats["total_size"] / stats["requests"] if stats["requests"] > 0 else 0
            
            return {
                "total_requests": total_requests,
                "average_response_time": avg_response_time,
                "protocol_stats": protocol_stats,
                "cache_stats": self.cache_stats
            }
    
    def cache_response(self, cache_key: str, response: Any, ttl: float = 300.0):
        """
        ç¼“å­˜å“åº”
        """
        with self.optimizer_lock:
            self.cache[cache_key] = {
                "response": response,
                "expiry": time.time() + ttl
            }
            self.cache_stats["misses"] += 1
    
    def get_cached_response(self, cache_key: str) -> Optional[Any]:
        """
        è·å–ç¼“å­˜å“åº”
        """
        with self.optimizer_lock:
            if cache_key in self.cache:
                cached_item = self.cache[cache_key]
                if time.time() < cached_item["expiry"]:
                    self.cache_stats["hits"] += 1
                    return cached_item["response"]
                else:
                    # ç¼“å­˜è¿‡æœŸï¼Œåˆ é™¤
                    del self.cache[cache_key]
            
            return None
    
    def rate_limit_check(self, client_id: str, max_requests: int = 100, 
                        time_window: float = 60.0) -> bool:
        """
        é€Ÿç‡é™åˆ¶æ£€æŸ¥
        """
        with self.optimizer_lock:
            current_time = time.time()
            
            if client_id not in self.rate_limiter:
                self.rate_limiter[client_id] = []
            
            # æ¸…ç†è¿‡æœŸçš„è¯·æ±‚è®°å½•
            requests = self.rate_limiter[client_id]
            requests[:] = [req_time for req_time in requests if current_time - req_time < time_window]
            
            # æ£€æŸ¥æ˜¯å¦è¶…è¿‡é™åˆ¶
            if len(requests) >= max_requests:
                return False  # è¶…è¿‡é™åˆ¶
            
            # è®°å½•å½“å‰è¯·æ±‚
            requests.append(current_time)
            return True  # å…è®¸è¯·æ±‚

class ConnectionPool:
    """
    è¿æ¥æ± ç®¡ç†
    """
    def __init__(self, max_connections: int = 100):
        self.max_connections = max_connections
        self.connections: Dict[str, List[Any]] = {}
        self.connection_lock = threading.Lock()
        self.stats = {
            "created": 0,
            "reused": 0,
            "closed": 0
        }
    
    def get_connection(self, connection_key: str) -> Any:
        """
        è·å–è¿æ¥
        """
        with self.connection_lock:
            if connection_key in self.connections and self.connections[connection_key]:
                # é‡ç”¨ç°æœ‰è¿æ¥
                conn = self.connections[connection_key].pop()
                self.stats["reused"] += 1
                return conn
            else:
                # åˆ›å»ºæ–°è¿æ¥
                conn = self._create_connection(connection_key)
                self.stats["created"] += 1
                return conn
    
    def return_connection(self, connection_key: str, connection: Any):
        """
        å½’è¿˜è¿æ¥
        """
        with self.connection_lock:
            if connection_key not in self.connections:
                self.connections[connection_key] = []
            
            # å¦‚æœè¿æ¥æ± æœªæ»¡ï¼Œå½’è¿˜è¿æ¥
            if len(self.connections[connection_key]) < self.max_connections:
                self.connections[connection_key].append(connection)
            else:
                # è¿æ¥æ± å·²æ»¡ï¼Œå…³é—­è¿æ¥
                self._close_connection(connection)
                self.stats["closed"] += 1
    
    def _create_connection(self, connection_key: str) -> Any:
        """
        åˆ›å»ºè¿æ¥ï¼ˆæ¨¡æ‹Ÿå®ç°ï¼‰
        """
        print(f"åˆ›å»ºæ–°è¿æ¥: {connection_key}")
        return f"connection_{connection_key}_{int(time.time() * 1000)}"
    
    def _close_connection(self, connection: Any):
        """
        å…³é—­è¿æ¥ï¼ˆæ¨¡æ‹Ÿå®ç°ï¼‰
        """
        print(f"å…³é—­è¿æ¥: {connection}")
    
    def get_stats(self) -> Dict[str, int]:
        """
        è·å–è¿æ¥æ± ç»Ÿè®¡ä¿¡æ¯
        """
        with self.connection_lock:
            total_connections = sum(len(conns) for conns in self.connections.values())
            return {
                **self.stats,
                "available": total_connections
            }

class AsyncRequestHandler:
    """
    å¼‚æ­¥è¯·æ±‚å¤„ç†å™¨
    """
    def __init__(self, gateway):
        self.gateway = gateway
        self.request_queue = asyncio.Queue()
        self.workers = []
        self.running = False
    
    async def start_workers(self, num_workers: int = 4):
        """
        å¯åŠ¨å·¥ä½œçº¿ç¨‹
        """
        self.running = True
        for i in range(num_workers):
            worker = asyncio.create_task(self._worker(f"worker-{i}"))
            self.workers.append(worker)
        print(f"å¯åŠ¨ {num_workers} ä¸ªå¼‚æ­¥å·¥ä½œçº¿ç¨‹")
    
    async def stop_workers(self):
        """
        åœæ­¢å·¥ä½œçº¿ç¨‹
        """
        self.running = False
        for worker in self.workers:
            worker.cancel()
        await asyncio.gather(*self.workers, return_exceptions=True)
        print("åœæ­¢æ‰€æœ‰å¼‚æ­¥å·¥ä½œçº¿ç¨‹")
    
    async def submit_request(self, request_data: Dict[str, Any]):
        """
        æäº¤è¯·æ±‚åˆ°é˜Ÿåˆ—
        """
        await self.request_queue.put(request_data)
    
    async def _worker(self, worker_id: str):
        """
        å·¥ä½œçº¿ç¨‹
        """
        print(f"å·¥ä½œçº¿ç¨‹ {worker_id} å¯åŠ¨")
        while self.running:
            try:
                # ä»é˜Ÿåˆ—è·å–è¯·æ±‚
                request_data = await asyncio.wait_for(self.request_queue.get(), timeout=1.0)
                
                # å¤„ç†è¯·æ±‚
                await self._process_request(request_data)
                
                # æ ‡è®°ä»»åŠ¡å®Œæˆ
                self.request_queue.task_done()
                
            except asyncio.TimeoutError:
                # è¶…æ—¶ç»§ç»­å¾ªç¯
                continue
            except Exception as e:
                print(f"å·¥ä½œçº¿ç¨‹ {worker_id} å¤„ç†è¯·æ±‚æ—¶å‡ºé”™: {e}")
                self.request_queue.task_done()
    
    async def _process_request(self, request_data: Dict[str, Any]):
        """
        å¤„ç†è¯·æ±‚
        """
        protocol = request_data.get("protocol", "unknown")
        method = request_data.get("method", "GET")
        path = request_data.get("path", "/")
        headers = request_data.get("headers", {})
        body = request_data.get("body", b"")
        
        print(f"å¼‚æ­¥å¤„ç†è¯·æ±‚: {protocol} {method} {path}")
        
        # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
        await asyncio.sleep(0.01)
        
        # è°ƒç”¨ç½‘å…³å¤„ç†è¯·æ±‚
        response = self.gateway.handle_request(protocol, method, path, headers, body)
        
        # åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šå°†å“åº”å‘é€å›å®¢æˆ·ç«¯
        print(f"å¼‚æ­¥è¯·æ±‚å¤„ç†å®Œæˆ: çŠ¶æ€ç  {response['status']}")

class OptimizedGateway:
    """
    ä¼˜åŒ–çš„ç½‘å…³å®ç°
    """
    def __init__(self, config: GatewayConfig):
        self.config = config
        self.performance_optimizer = PerformanceOptimizer(self)
        self.connection_pool = ConnectionPool(max_connections=50)
        self.async_handler = AsyncRequestHandler(self)
        self.running = False
    
    def start(self):
        """
        å¯åŠ¨ä¼˜åŒ–çš„ç½‘å…³
        """
        self.running = True
        print("å¯åŠ¨ä¼˜åŒ–çš„åè®®ç½‘å…³...")
        
        # å¯åŠ¨å¼‚æ­¥å¤„ç†å·¥ä½œçº¿ç¨‹
        asyncio.run(self.async_handler.start_workers(4))
    
    def stop(self):
        """
        åœæ­¢ç½‘å…³
        """
        self.running = False
        print("åœæ­¢ä¼˜åŒ–çš„åè®®ç½‘å…³...")
        
        # åœæ­¢å¼‚æ­¥å¤„ç†å·¥ä½œçº¿ç¨‹
        asyncio.run(self.async_handler.stop_workers())
    
    def handle_request(self, protocol: str, method: str, path: str,
                      headers: Dict[str, str], body: bytes = None) -> Dict[str, Any]:
        """
        å¤„ç†è¯·æ±‚ï¼ˆå¸¦æ€§èƒ½ä¼˜åŒ–ï¼‰
        """
        request_id = f"req_{int(time.time() * 1000000)}"
        
        # è®°å½•è¯·æ±‚å¼€å§‹
        metrics = self.performance_optimizer.record_request_start(
            request_id, protocol, method, path
        )
        
        try:
            # é€Ÿç‡é™åˆ¶æ£€æŸ¥
            client_id = headers.get("X-Client-ID", "unknown")
            if not self.performance_optimizer.rate_limit_check(client_id):
                response = {
                    "status": 429,  # Too Many Requests
                    "headers": {},
                    "body": b'{"error": "Rate limit exceeded"}'
                }
            else:
                # æ£€æŸ¥ç¼“å­˜
                cache_key = f"{protocol}:{method}:{path}"
                cached_response = self.performance_optimizer.get_cached_response(cache_key)
                
                if cached_response:
                    response = cached_response
                else:
                    # ä»è¿æ¥æ± è·å–è¿æ¥
                    conn_key = f"{protocol}_backend"
                    connection = self.connection_pool.get_connection(conn_key)
                    
                    try:
                        # å®é™…å¤„ç†è¯·æ±‚ï¼ˆè¿™é‡Œç®€åŒ–å¤„ç†ï¼‰
                        response = {
                            "status": 200,
                            "headers": {"Content-Type": "application/json"},
                            "body": b'{"message": "Request processed successfully"}'
                        }
                        
                        # ç¼“å­˜å“åº”ï¼ˆå¯¹äºGETè¯·æ±‚ï¼‰
                        if method == "GET":
                            self.performance_optimizer.cache_response(cache_key, response)
                    
                    finally:
                        # å½’è¿˜è¿æ¥åˆ°è¿æ¥æ± 
                        self.connection_pool.return_connection(conn_key, connection)
            
            # è®°å½•è¯·æ±‚ç»“æŸ
            self.performance_optimizer.record_request_end(
                metrics, response["status"], len(response["body"])
            )
            
            return response
        
        except Exception as e:
            # è®°å½•é”™è¯¯è¯·æ±‚
            self.performance_optimizer.record_request_end(metrics, 500, 0)
            print(f"å¤„ç†è¯·æ±‚æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return {
                "status": 500,
                "headers": {},
                "body": b'{"error": "Internal server error"}'
            }
    
    async def handle_async_request(self, request_data: Dict[str, Any]):
        """
        å¼‚æ­¥å¤„ç†è¯·æ±‚
        """
        await self.async_handler.submit_request(request_data)
    
    def get_performance_stats(self) -> Dict[str, Any]:
        """
        è·å–æ€§èƒ½ç»Ÿè®¡
        """
        gateway_stats = self.performance_optimizer.get_performance_stats()
        connection_stats = self.connection_pool.get_stats()
        
        return {
            "gateway_stats": gateway_stats,
            "connection_pool_stats": connection_stats
        }

# ä½¿ç”¨ç¤ºä¾‹
print("\n=== ç½‘å…³æ€§èƒ½ä¼˜åŒ–æ¼”ç¤º ===")

# åˆ›å»ºä¼˜åŒ–çš„ç½‘å…³
config = GatewayConfig(
    host="0.0.0.0",
    port=8080,
    enable_hdfs=True,
    enable_s3=True
)

optimized_gateway = OptimizedGateway(config)

# å¯åŠ¨ç½‘å…³
print("å¯åŠ¨ä¼˜åŒ–çš„ç½‘å…³...")
optimized_gateway.start()

# æ¨¡æ‹Ÿå¤„ç†å¤§é‡è¯·æ±‚
print("\næ¨¡æ‹Ÿå¤„ç†å¤§é‡è¯·æ±‚...")
import concurrent.futures

def simulate_request(gateway, request_num):
    """æ¨¡æ‹Ÿå•ä¸ªè¯·æ±‚"""
    protocol = "s3" if request_num % 2 == 0 else "hdfs"
    method = "GET" if request_num % 3 == 0 else "POST"
    path = f"/bucket/object_{request_num}.txt"
    
    response = gateway.handle_request(
        protocol=protocol,
        method=method,
        path=path,
        headers={
            "X-Client-ID": f"client_{request_num % 10}",
            "Authorization": "Bearer token123"
        },
        body=b"Test data"
    )
    
    return response["status"]

# ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘å¤„ç†è¯·æ±‚
with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
    # æäº¤100ä¸ªè¯·æ±‚
    futures = [executor.submit(simulate_request, optimized_gateway, i) for i in range(100)]
    
    # è·å–ç»“æœ
    results = [future.result() for future in concurrent.futures.as_completed(futures)]
    
    success_count = sum(1 for status in results if status == 200)
    print(f"å¤„ç†äº† {len(results)} ä¸ªè¯·æ±‚ï¼ŒæˆåŠŸ {success_count} ä¸ª")

# æŸ¥çœ‹æ€§èƒ½ç»Ÿè®¡
print("\næ€§èƒ½ç»Ÿè®¡ä¿¡æ¯:")
stats = optimized_gateway.get_performance_stats()
gateway_stats = stats["gateway_stats"]
connection_stats = stats["connection_pool_stats"]

print(f"ç½‘å…³ç»Ÿè®¡:")
print(f"  æ€»è¯·æ±‚æ•°: {gateway_stats.get('total_requests', 0)}")
print(f"  å¹³å‡å“åº”æ—¶é—´: {gateway_stats.get('average_response_time', 0):.4f}ç§’")
print(f"  ç¼“å­˜å‘½ä¸­: {gateway_stats.get('cache_stats', {}).get('hits', 0)}")
print(f"  ç¼“å­˜æœªå‘½ä¸­: {gateway_stats.get('cache_stats', {}).get('misses', 0)}")

print(f"è¿æ¥æ± ç»Ÿè®¡:")
for key, value in connection_stats.items():
    print(f"  {key}: {value}")

# æŒ‰åè®®æŸ¥çœ‹è¯¦ç»†ç»Ÿè®¡
if "protocol_stats" in gateway_stats:
    print(f"åè®®è¯¦ç»†ç»Ÿè®¡:")
    for protocol, proto_stats in gateway_stats["protocol_stats"].items():
        print(f"  {protocol}:")
        print(f"    è¯·æ±‚æ•°: {proto_stats['requests']}")
        print(f"    å¹³å‡å“åº”æ—¶é—´: {proto_stats['avg_response_time']:.4f}ç§’")
        print(f"    é”™è¯¯ç‡: {proto_stats['error_rate']:.2%}")
        print(f"    å¹³å‡å“åº”å¤§å°: {proto_stats['avg_response_size']:.0f}å­—èŠ‚")

# åœæ­¢ç½‘å…³
print("\nåœæ­¢ä¼˜åŒ–çš„ç½‘å…³...")
optimized_gateway.stop()
```

## æ€»ç»“

ä¸HDFSã€S3ç­‰æ ‡å‡†åè®®çš„å…¼å®¹ä¸ç½‘å…³æ„å»ºæ˜¯åˆ†å¸ƒå¼æ–‡ä»¶å­˜å‚¨å¹³å°æˆåŠŸçš„å…³é”®å› ç´ ã€‚é€šè¿‡æœ¬ç« çš„æ·±å…¥æ¢è®¨ï¼Œæˆ‘ä»¬äº†è§£äº†ä»¥ä¸‹æ ¸å¿ƒå†…å®¹ï¼š

1. **åè®®å…¼å®¹æ€§æ¦‚è¿°**ï¼šåè®®å…¼å®¹æ€§å¯¹äºå¹³å°çš„ç”Ÿæ€ç³»ç»Ÿé›†æˆå’Œç”¨æˆ·é‡‡ç”¨ç‡å…·æœ‰é‡è¦å½±å“ï¼Œéœ€è¦æ”¯æŒHDFSã€S3ã€POSIXç­‰å¤šç§æ ‡å‡†åè®®ã€‚

2. **HDFSåè®®å…¼å®¹å®ç°**ï¼šé€šè¿‡å®ç°å®Œæ•´çš„HDFS APIå…¼å®¹å±‚ï¼ŒåŒ…æ‹¬æ–‡ä»¶åˆ›å»ºã€è¯»å†™ã€åˆ é™¤ã€ç›®å½•æ“ä½œç­‰åŠŸèƒ½ï¼Œç¡®ä¿ä¸Hadoopç”Ÿæ€ç³»ç»Ÿçš„æ— ç¼é›†æˆã€‚

3. **S3åè®®å…¼å®¹å®ç°**ï¼šé€šè¿‡å®ç°RESTful S3 APIå…¼å®¹å±‚ï¼ŒåŒ…æ‹¬å¯¹è±¡å­˜å‚¨ã€åˆ†æ®µä¸Šä¼ ã€å…ƒæ•°æ®ç®¡ç†ç­‰åŠŸèƒ½ï¼Œç¡®ä¿ä¸äº‘åŸç”Ÿåº”ç”¨çš„å¤©ç„¶å…¼å®¹ã€‚

4. **ç½‘å…³æ„å»ºä¸é›†æˆ**ï¼šé€šè¿‡æ„å»ºç»Ÿä¸€çš„åè®®ç½‘å…³ï¼Œæ”¯æŒå¤šåè®®æ¥å…¥ã€è´Ÿè½½å‡è¡¡ã€æ€§èƒ½ä¼˜åŒ–ç­‰é«˜çº§åŠŸèƒ½ï¼Œä¸ºç”¨æˆ·æä¾›ä¸€è‡´çš„è®¿é—®ä½“éªŒã€‚

åœ¨å®é™…å·¥ç¨‹å®è·µä¸­ï¼Œéœ€è¦æ³¨æ„ä»¥ä¸‹è¦ç‚¹ï¼š

- **å…¼å®¹æ€§æµ‹è¯•**ï¼šå»ºç«‹å®Œå–„çš„å…¼å®¹æ€§æµ‹è¯•æ¡†æ¶ï¼Œç¡®ä¿ä¸æ ‡å‡†åè®®çš„å®Œå…¨å…¼å®¹
- **æ€§èƒ½ä¼˜åŒ–**ï¼šé€šè¿‡ç¼“å­˜ã€è¿æ¥æ± ã€å¼‚æ­¥å¤„ç†ç­‰æŠ€æœ¯ä¼˜åŒ–ç½‘å…³æ€§èƒ½
- **ç›‘æ§ä¸è¿ç»´**ï¼šå»ºç«‹å…¨é¢çš„ç›‘æ§ä½“ç³»ï¼Œå®æ—¶è·Ÿè¸ªç½‘å…³æ€§èƒ½å’Œå¥åº·çŠ¶æ€
- **å®‰å…¨è€ƒè™‘**ï¼šå®ç°å®Œå–„çš„èº«ä»½è®¤è¯å’Œè®¿é—®æ§åˆ¶æœºåˆ¶ï¼Œç¡®ä¿æ•°æ®å®‰å…¨

é€šè¿‡åˆç†è®¾è®¡å’Œå®ç°è¿™äº›å…¼å®¹æ€§æœºåˆ¶ï¼Œåˆ†å¸ƒå¼æ–‡ä»¶å­˜å‚¨å¹³å°èƒ½å¤Ÿæ— ç¼é›†æˆåˆ°ç°æœ‰çš„å¤§æ•°æ®å’Œäº‘åŸç”Ÿç”Ÿæ€ç³»ç»Ÿä¸­ï¼Œä¸ºç”¨æˆ·æä¾›çµæ´»å¤šæ ·çš„æ•°æ®è®¿é—®æ–¹å¼ã€‚
