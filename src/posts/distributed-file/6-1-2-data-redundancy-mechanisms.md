---
title: 数据冗余机制：多副本（Replication）的实现与调度
date: 2025-09-07
categories: [DFS]
tags: [dfs, replication, redundancy]
published: true
---

在分布式文件存储系统中，数据冗余是保障数据可靠性和系统高可用性的核心技术之一。通过在多个节点上存储数据的副本，系统可以在部分节点发生故障时仍然保证数据的完整性和可访问性。本章将深入探讨多副本机制的实现原理、调度策略以及在分布式文件存储平台中的具体应用。

## 6.1.2.1 数据冗余机制概述

数据冗余是分布式存储系统中用于提高数据可靠性和可用性的关键技术。通过在多个位置存储相同的数据，系统可以在部分存储节点发生故障时仍然保证数据的完整性和可访问性。

### 6.1.2.1.1 冗余机制的分类

在分布式存储系统中，主要有以下几种数据冗余机制：

1. **多副本机制（Replication）**：在多个节点上存储完全相同的数据副本
2. **纠删码机制（Erasure Coding）**：将数据编码为多个数据块和校验块，通过部分块即可恢复原始数据
3. **混合冗余机制**：结合多副本和纠删码的优点，根据数据重要性和访问模式选择合适的冗余策略

### 6.1.2.1.2 冗余机制的选择因素

在选择数据冗余机制时，需要考虑以下因素：

1. **数据重要性**：关键数据需要更高的可靠性保障
2. **访问频率**：热数据需要更快的访问速度
3. **存储成本**：不同冗余机制的存储开销不同
4. **恢复时间**：数据恢复所需的时间和资源
5. **系统复杂度**：实现和维护的复杂程度

## 6.1.2.2 多副本机制原理

多副本机制是最常见和最直观的数据冗余方式，通过在多个节点上存储相同的数据副本，实现数据的高可用性和容错能力。

### 6.1.2.2.1 副本数量策略

副本数量是多副本机制的核心参数，直接影响系统的可靠性和存储成本。

```python
# 副本数量策略示例
import math
from typing import Dict, List

class ReplicaStrategy:
    def __init__(self):
        # 不同数据重要性的默认副本数
        self.importance_replicas = {
            "critical": 5,    # 关键数据，5副本
            "high": 3,        # 高重要性数据，3副本
            "medium": 2,      # 中等重要性数据，2副本
            "low": 1          # 低重要性数据，1副本
        }
    
    def calculate_replicas_by_rack_awareness(self, total_racks: int, min_racks: int = 2) -> int:
        """根据机架感知计算副本数"""
        # 至少需要分布在min_racks个不同的机架上
        return max(min_racks, min(total_racks, 3))
    
    def calculate_replicas_by_availability_target(self, target_availability: float, 
                                                 node_failure_rate: float = 0.01) -> int:
        """根据可用性目标计算副本数"""
        # 简化的计算模型：(1 - node_failure_rate)^replicas >= target_availability
        if target_availability >= 1:
            return 5  # 最大副本数
        
        replicas = math.ceil(math.log(1 - target_availability) / math.log(1 - node_failure_rate))
        return max(1, min(replicas, 5))  # 限制在1-5范围内
    
    def get_replica_count(self, data_importance: str, total_racks: int = 3, 
                         target_availability: float = 0.999) -> Dict[str, int]:
        """获取综合副本数策略"""
        importance_replicas = self.importance_replicas.get(data_importance, 2)
        rack_aware_replicas = self.calculate_replicas_by_rack_awareness(total_racks)
        availability_replicas = self.calculate_replicas_by_availability_target(target_availability)
        
        # 综合考虑各种因素
        final_replicas = max(importance_replicas, rack_aware_replicas, availability_replicas)
        return {
            "importance_based": importance_replicas,
            "rack_aware_based": rack_aware_replicas,
            "availability_based": availability_replicas,
            "final_count": min(final_replicas, 5)  # 最多5个副本
        }

# 使用示例
strategy = ReplicaStrategy()

# 不同场景下的副本数计算
scenarios = [
    {"importance": "critical", "racks": 5, "availability": 0.9999},
    {"importance": "high", "racks": 3, "availability": 0.999},
    {"importance": "medium", "racks": 2, "availability": 0.99},
    {"importance": "low", "racks": 1, "availability": 0.95}
]

for scenario in scenarios:
    result = strategy.get_replica_count(
        scenario["importance"], 
        scenario["racks"], 
        scenario["availability"]
    )
    print(f"场景: {scenario}")
    print(f"副本策略: {result}")
    print("---")
```

### 6.1.2.2.2 副本放置策略

副本放置策略决定了副本在集群中的分布位置，直接影响系统的容错能力和负载均衡。

```python
# 副本放置策略示例
import random
from typing import List, Dict, Set

class ReplicaPlacement:
    def __init__(self, nodes: List[Dict]):
        self.nodes = nodes  # 节点信息列表，包含id、rack、capacity等信息
    
    def rack_aware_placement(self, primary_node_id: str, replica_count: int) -> List[str]:
        """机架感知放置策略"""
        # 找到主节点信息
        primary_node = next((node for node in self.nodes if node["id"] == primary_node_id), None)
        if not primary_node:
            raise ValueError("Primary node not found")
        
        primary_rack = primary_node["rack"]
        
        # 按机架分组节点
        rack_nodes = {}
        for node in self.nodes:
            if node["id"] != primary_node_id:  # 排除主节点
                rack = node["rack"]
                if rack not in rack_nodes:
                    rack_nodes[rack] = []
                rack_nodes[rack].append(node["id"])
        
        # 选择副本节点
        selected_nodes = []
        racks_used = {primary_rack}  # 已使用的机架
        
        # 首先尝试在不同机架上放置副本
        available_racks = [rack for rack in rack_nodes.keys() if rack != primary_rack]
        random.shuffle(available_racks)
        
        for rack in available_racks:
            if len(selected_nodes) >= replica_count - 1:
                break
            # 从该机架中随机选择一个节点
            if rack_nodes[rack]:
                selected_node = random.choice(rack_nodes[rack])
                selected_nodes.append(selected_node)
                racks_used.add(rack)
        
        # 如果副本数还不够，从已使用的机架中选择
        if len(selected_nodes) < replica_count - 1:
            remaining_count = replica_count - 1 - len(selected_nodes)
            # 收集所有可用节点（排除主节点）
            all_available_nodes = [node["id"] for node in self.nodes if node["id"] != primary_node_id]
            # 排除已选择的节点
            for selected in selected_nodes:
                if selected in all_available_nodes:
                    all_available_nodes.remove(selected)
            
            # 随机选择剩余节点
            additional_nodes = random.sample(all_available_nodes, min(remaining_count, len(all_available_nodes)))
            selected_nodes.extend(additional_nodes)
        
        # 返回主节点和副本节点
        return [primary_node_id] + selected_nodes
    
    def capacity_aware_placement(self, replica_count: int, data_size: int) -> List[str]:
        """容量感知放置策略"""
        # 按剩余容量排序节点
        sorted_nodes = sorted(self.nodes, key=lambda x: x.get("available_capacity", 0), reverse=True)
        
        # 选择前replica_count个节点
        selected_nodes = [node["id"] for node in sorted_nodes[:replica_count]]
        return selected_nodes
    
    def load_balanced_placement(self, replica_count: int) -> List[str]:
        """负载均衡放置策略"""
        # 按当前负载排序节点（负载越低越优先）
        sorted_nodes = sorted(self.nodes, key=lambda x: x.get("current_load", 0))
        
        # 选择前replica_count个节点
        selected_nodes = [node["id"] for node in sorted_nodes[:replica_count]]
        return selected_nodes
    
    def hybrid_placement(self, primary_node_id: str, replica_count: int, data_size: int) -> List[str]:
        """混合放置策略：综合考虑机架感知、容量和负载"""
        # 找到主节点信息
        primary_node = next((node for node in self.nodes if node["id"] == primary_node_id), None)
        if not primary_node:
            # 如果没有指定主节点，选择一个合适的节点作为主节点
            primary_node = max(self.nodes, key=lambda x: x.get("available_capacity", 0))
            primary_node_id = primary_node["id"]
        
        # 按综合评分排序（容量权重0.4，负载权重0.4，机架权重0.2）
        def node_score(node):
            capacity_score = node.get("available_capacity", 0) / 1000000  # 假设容量以MB为单位
            load_score = (100 - node.get("current_load", 0)) / 100  # 负载越低得分越高
            rack_score = 1 if node["rack"] != primary_node["rack"] else 0.5  # 不同机架得分更高
            return capacity_score * 0.4 + load_score * 0.4 + rack_score * 0.2
        
        # 排除主节点后排序
        candidate_nodes = [node for node in self.nodes if node["id"] != primary_node_id]
        sorted_nodes = sorted(candidate_nodes, key=node_score, reverse=True)
        
        # 选择前replica_count-1个节点作为副本
        replica_nodes = [node["id"] for node in sorted_nodes[:replica_count-1]]
        return [primary_node_id] + replica_nodes

# 使用示例
# 模拟节点信息
nodes = [
    {"id": "node1", "rack": "rack1", "available_capacity": 1000000, "current_load": 30},
    {"id": "node2", "rack": "rack1", "available_capacity": 800000, "current_load": 50},
    {"id": "node3", "rack": "rack2", "available_capacity": 1200000, "current_load": 20},
    {"id": "node4", "rack": "rack2", "available_capacity": 900000, "current_load": 40},
    {"id": "node5", "rack": "rack3", "available_capacity": 1100000, "current_load": 25},
    {"id": "node6", "rack": "rack3", "available_capacity": 700000, "current_load": 60},
]

placement = ReplicaPlacement(nodes)

# 机架感知放置
rack_aware_result = placement.rack_aware_placement("node1", 3)
print(f"机架感知放置结果: {rack_aware_result}")

# 容量感知放置
capacity_aware_result = placement.capacity_aware_placement(3, 100000)
print(f"容量感知放置结果: {capacity_aware_result}")

# 负载均衡放置
load_balanced_result = placement.load_balanced_placement(3)
print(f"负载均衡放置结果: {load_balanced_result}")

# 混合放置策略
hybrid_result = placement.hybrid_placement("node1", 3, 100000)
print(f"混合放置策略结果: {hybrid_result}")
```

## 6.1.2.3 多副本同步机制

多副本同步机制确保所有副本之间数据的一致性，是多副本机制的核心组成部分。

### 6.1.2.3.1 同步模式

```python
# 多副本同步机制示例
import threading
import time
from enum import Enum
from typing import List, Dict, Callable
import hashlib

class SyncMode(Enum):
    SYNC = "sync"           # 同步写入，所有副本写入成功才算成功
    ASYNC = "async"         # 异步写入，主副本写入成功就算成功
    QUORUM = "quorum"       # 法定数量写入，达到法定数量就算成功

class ReplicaSyncManager:
    def __init__(self, replicas: List[str]):
        self.replicas = replicas
        self.quorum_size = len(replicas) // 2 + 1  # 简单的法定数量计算
    
    def sync_write(self, data: bytes, sync_mode: SyncMode = SyncMode.QUORUM) -> Dict:
        """同步写入数据到副本"""
        data_hash = hashlib.md5(data).hexdigest()
        timestamp = time.time()
        
        if sync_mode == SyncMode.SYNC:
            return self._sync_write_all(data, data_hash, timestamp)
        elif sync_mode == SyncMode.ASYNC:
            return self._async_write(data, data_hash, timestamp)
        elif sync_mode == SyncMode.QUORUM:
            return self._quorum_write(data, data_hash, timestamp)
        else:
            raise ValueError("Invalid sync mode")
    
    def _sync_write_all(self, data: bytes, data_hash: str, timestamp: float) -> Dict:
        """同步写入所有副本"""
        start_time = time.time()
        results = {}
        success_count = 0
        
        # 同时向所有副本写入数据
        threads = []
        thread_results = {}
        
        def write_to_replica(replica_id):
            try:
                # 模拟写入操作
                time.sleep(random.uniform(0.01, 0.1))  # 模拟网络延迟
                # 模拟写入成功率
                if random.random() > 0.1:  # 90%成功率
                    thread_results[replica_id] = {
                        "success": True,
                        "hash": data_hash,
                        "timestamp": timestamp
                    }
                else:
                    thread_results[replica_id] = {
                        "success": False,
                        "error": "Write failed"
                    }
            except Exception as e:
                thread_results[replica_id] = {
                    "success": False,
                    "error": str(e)
                }
        
        # 启动写入线程
        for replica_id in self.replicas:
            thread = threading.Thread(target=write_to_replica, args=(replica_id,))
            threads.append(thread)
            thread.start()
        
        # 等待所有线程完成
        for thread in threads:
            thread.join()
        
        results = thread_results
        success_count = sum(1 for r in results.values() if r.get("success", False))
        
        total_time = time.time() - start_time
        return {
            "mode": "sync",
            "success": success_count == len(self.replicas),
            "success_count": success_count,
            "total_replicas": len(self.replicas),
            "results": results,
            "time_cost": total_time
        }
    
    def _async_write(self, data: bytes, data_hash: str, timestamp: float) -> Dict:
        """异步写入数据"""
        start_time = time.time()
        
        # 首先写入主副本
        primary_replica = self.replicas[0]
        try:
            # 模拟主副本写入
            time.sleep(random.uniform(0.005, 0.02))  # 主副本写入较快
            primary_success = random.random() > 0.05  # 95%成功率
            
            if primary_success:
                primary_result = {
                    "success": True,
                    "hash": data_hash,
                    "timestamp": timestamp
                }
            else:
                primary_result = {
                    "success": False,
                    "error": "Primary write failed"
                }
        except Exception as e:
            primary_result = {
                "success": False,
                "error": str(e)
            }
        
        # 异步写入其他副本
        def async_write_to_replica(replica_id):
            try:
                time.sleep(random.uniform(0.02, 0.05))  # 异步写入延迟更高
                if random.random() > 0.15:  # 85%成功率
                    return {
                        "success": True,
                        "hash": data_hash,
                        "timestamp": timestamp
                    }
                else:
                    return {
                        "success": False,
                        "error": "Async write failed"
                    }
            except Exception as e:
                return {
                    "success": False,
                    "error": str(e)
                }
        
        # 启动异步写入线程（不等待完成）
        async_results = {}
        for replica_id in self.replicas[1:]:
            thread = threading.Thread(
                target=lambda rid: async_results.update({rid: async_write_to_replica(rid)}), 
                args=(replica_id,)
            )
            thread.daemon = True  # 设置为守护线程
            thread.start()
        
        total_time = time.time() - start_time
        return {
            "mode": "async",
            "success": primary_result["success"],
            "primary_result": primary_result,
            "async_results": "Writing in background",
            "time_cost": total_time
        }
    
    def _quorum_write(self, data: bytes, data_hash: str, timestamp: float) -> Dict:
        """法定数量写入"""
        start_time = time.time()
        results = {}
        success_count = 0
        completed_count = 0
        
        # 使用信号量控制并发
        semaphore = threading.Semaphore(3)  # 最多3个并发写入
        
        def write_to_replica(replica_id, result_dict):
            nonlocal success_count, completed_count
            semaphore.acquire()
            try:
                time.sleep(random.uniform(0.01, 0.08))  # 模拟写入延迟
                success = random.random() > 0.1  # 90%成功率
                
                result = {
                    "success": success,
                    "hash": data_hash,
                    "timestamp": timestamp
                } if success else {
                    "success": False,
                    "error": "Write failed"
                }
                
                result_dict[replica_id] = result
                
                if success:
                    success_count += 1
                completed_count += 1
            finally:
                semaphore.release()
        
        # 启动写入线程
        threads = []
        for replica_id in self.replicas:
            thread = threading.Thread(target=write_to_replica, args=(replica_id, results))
            threads.append(thread)
            thread.start()
        
        # 等待达到法定数量或所有完成
        while success_count < self.quorum_size and completed_count < len(self.replicas):
            time.sleep(0.001)  # 短暂休眠避免忙等待
        
        # 如果达到法定数量，中断其他未完成的写入
        total_time = time.time() - start_time
        return {
            "mode": "quorum",
            "success": success_count >= self.quorum_size,
            "success_count": success_count,
            "quorum_size": self.quorum_size,
            "total_replicas": len(self.replicas),
            "results": results,
            "time_cost": total_time
        }

# 使用示例
replicas = ["replica1", "replica2", "replica3", "replica4", "replica5"]
sync_manager = ReplicaSyncManager(replicas)

# 测试数据
test_data = b"This is test data for replica synchronization." * 100  # 增大数据量

# 同步写入
print("=== 同步写入测试 ===")
sync_result = sync_manager.sync_write(test_data, SyncMode.SYNC)
print(f"同步写入结果: {sync_result['success']}")
print(f"成功副本数: {sync_result['success_count']}/{sync_result['total_replicas']}")
print(f"耗时: {sync_result['time_cost']:.4f}秒")

# 异步写入
print("\n=== 异步写入测试 ===")
async_result = sync_manager.sync_write(test_data, SyncMode.ASYNC)
print(f"异步写入结果: {async_result['success']}")
print(f"主副本结果: {async_result['primary_result']['success']}")
print(f"耗时: {async_result['time_cost']:.4f}秒")

# 法定数量写入
print("\n=== 法定数量写入测试 ===")
quorum_result = sync_manager.sync_write(test_data, SyncMode.QUORUM)
print(f"法定数量写入结果: {quorum_result['success']}")
print(f"成功副本数: {quorum_result['success_count']}/{quorum_result['quorum_size']}")
print(f"耗时: {quorum_result['time_cost']:.4f}秒")
```

### 6.1.2.3.2 副本一致性维护

```python
# 副本一致性维护示例
import hashlib
import time
from typing import Dict, List, Tuple
import threading

class ReplicaConsistencyManager:
    def __init__(self, replicas: List[str]):
        self.replicas = replicas
        self.replica_metadata = {replica: {} for replica in replicas}  # 存储每个副本的元数据
        self.consistency_lock = threading.Lock()
    
    def update_metadata(self, replica_id: str, data_id: str, data_hash: str, timestamp: float):
        """更新副本元数据"""
        with self.consistency_lock:
            if replica_id not in self.replica_metadata:
                self.replica_metadata[replica_id] = {}
            
            self.replica_metadata[replica_id][data_id] = {
                "hash": data_hash,
                "timestamp": timestamp,
                "last_check": time.time()
            }
    
    def check_consistency(self, data_id: str) -> Dict:
        """检查数据一致性"""
        with self.consistency_lock:
            results = {}
            hash_values = {}
            
            # 收集所有副本的元数据
            for replica_id in self.replicas:
                if (data_id in self.replica_metadata[replica_id] and
                    "hash" in self.replica_metadata[replica_id][data_id]):
                    hash_value = self.replica_metadata[replica_id][data_id]["hash"]
                    timestamp = self.replica_metadata[replica_id][data_id]["timestamp"]
                    results[replica_id] = {
                        "consistent": True,
                        "hash": hash_value,
                        "timestamp": timestamp
                    }
                    if hash_value not in hash_values:
                        hash_values[hash_value] = []
                    hash_values[hash_value].append(replica_id)
                else:
                    results[replica_id] = {
                        "consistent": False,
                        "error": "Missing data"
                    }
            
            # 检查一致性
            if len(hash_values) == 0:
                return {
                    "data_id": data_id,
                    "consistent": False,
                    "error": "No data found in any replica",
                    "details": results
                }
            elif len(hash_values) == 1:
                # 所有副本数据一致
                consistent_hash = list(hash_values.keys())[0]
                return {
                    "data_id": data_id,
                    "consistent": True,
                    "hash": consistent_hash,
                    "consistent_replicas": hash_values[consistent_hash],
                    "inconsistent_replicas": [],
                    "details": results
                }
            else:
                # 存在不一致
                # 找到最常见的哈希值作为正确值
                most_common_hash = max(hash_values.keys(), key=lambda x: len(hash_values[x]))
                consistent_replicas = hash_values[most_common_hash]
                inconsistent_replicas = []
                
                for hash_value, replicas in hash_values.items():
                    if hash_value != most_common_hash:
                        inconsistent_replicas.extend(replicas)
                
                return {
                    "data_id": data_id,
                    "consistent": False,
                    "correct_hash": most_common_hash,
                    "consistent_replicas": consistent_replicas,
                    "inconsistent_replicas": inconsistent_replicas,
                    "details": results
                }
    
    def repair_inconsistency(self, data_id: str, correct_hash: str) -> Dict:
        """修复数据不一致"""
        consistency_result = self.check_consistency(data_id)
        if consistency_result["consistent"]:
            return {
                "data_id": data_id,
                "repaired": False,
                "message": "Data is already consistent"
            }
        
        inconsistent_replicas = consistency_result["inconsistent_replicas"]
        repair_results = {}
        
        # 模拟从一致的副本复制数据到不一致的副本
        source_replica = consistency_result["consistent_replicas"][0] if consistency_result["consistent_replicas"] else None
        
        if not source_replica:
            return {
                "data_id": data_id,
                "repaired": False,
                "error": "No consistent source replica found"
            }
        
        for replica_id in inconsistent_replicas:
            try:
                # 模拟数据复制过程
                time.sleep(random.uniform(0.01, 0.05))  # 模拟网络延迟
                
                # 更新元数据
                self.update_metadata(
                    replica_id, 
                    data_id, 
                    correct_hash, 
                    time.time()
                )
                
                repair_results[replica_id] = {
                    "success": True,
                    "source": source_replica,
                    "time": time.time()
                }
            except Exception as e:
                repair_results[replica_id] = {
                    "success": False,
                    "error": str(e)
                }
        
        # 重新检查一致性
        final_check = self.check_consistency(data_id)
        
        return {
            "data_id": data_id,
            "repaired": True,
            "repair_results": repair_results,
            "final_consistency": final_check["consistent"]
        }
    
    def background_consistency_check(self, check_interval: float = 30.0):
        """后台一致性检查"""
        def check_loop():
            while True:
                try:
                    # 这里应该遍历所有数据项进行一致性检查
                    # 为简化示例，我们只检查一个测试数据
                    time.sleep(check_interval)
                except Exception as e:
                    print(f"Background consistency check error: {e}")
        
        # 启动后台检查线程
        check_thread = threading.Thread(target=check_loop, daemon=True)
        check_thread.start()
        return check_thread

# 使用示例
replicas = ["replica1", "replica2", "replica3", "replica4", "replica5"]
consistency_manager = ReplicaConsistencyManager(replicas)

# 模拟数据写入和元数据更新
test_data_id = "data_001"
test_hash = hashlib.md5(b"test data").hexdigest()
timestamp = time.time()

# 更新几个副本的元数据（模拟一致的情况）
consistency_manager.update_metadata("replica1", test_data_id, test_hash, timestamp)
consistency_manager.update_metadata("replica2", test_data_id, test_hash, timestamp)
consistency_manager.update_metadata("replica3", test_data_id, test_hash, timestamp)

# 检查一致性（应该是一致的）
result1 = consistency_manager.check_consistency(test_data_id)
print("=== 一致性检查（一致情况） ===")
print(f"数据ID: {result1['data_id']}")
print(f"是否一致: {result1['consistent']}")
if result1['consistent']:
    print(f"一致的副本: {result1['consistent_replicas']}")

# 模拟不一致情况
different_hash = hashlib.md5(b"different test data").hexdigest()
consistency_manager.update_metadata("replica4", test_data_id, different_hash, timestamp)
consistency_manager.update_metadata("replica5", test_data_id, different_hash, timestamp)

# 检查一致性（应该是不一致的）
result2 = consistency_manager.check_consistency(test_data_id)
print("\n=== 一致性检查（不一致情况） ===")
print(f"数据ID: {result2['data_id']}")
print(f"是否一致: {result2['consistent']}")
if not result2['consistent']:
    print(f"正确的哈希值: {result2['correct_hash']}")
    print(f"一致的副本: {result2['consistent_replicas']}")
    print(f"不一致的副本: {result2['inconsistent_replicas']}")

# 修复不一致
print("\n=== 修复不一致 ===")
repair_result = consistency_manager.repair_inconsistency(test_data_id, test_hash)
print(f"修复结果: {repair_result['repaired']}")
print(f"最终一致性: {repair_result['final_consistency']}")

# 重新检查一致性
result3 = consistency_manager.check_consistency(test_data_id)
print("\n=== 修复后一致性检查 ===")
print(f"是否一致: {result3['consistent']}")
if result3['consistent']:
    print(f"一致的副本: {result3['consistent_replicas']}")
```

## 6.1.2.4 副本调度与管理

副本调度和管理是确保系统高效运行的重要环节，包括副本的创建、删除、迁移等操作。

### 6.1.2.4.1 副本调度策略

```python
# 副本调度管理示例
import random
import time
from typing import Dict, List, Tuple
from dataclasses import dataclass
from enum import Enum

class ReplicaAction(Enum):
    CREATE = "create"
    DELETE = "delete"
    MIGRATE = "migrate"

@dataclass
class NodeInfo:
    id: str
    rack: str
    capacity: int
    available_capacity: int
    load: float
    status: str  # "healthy", "degraded", "failed"

@dataclass
class DataInfo:
    id: str
    size: int
    importance: str  # "critical", "high", "medium", "low"
    access_frequency: float  # 访问频率，0-1之间
    replicas: List[str]  # 当前副本所在的节点

class ReplicaScheduler:
    def __init__(self, nodes: List[NodeInfo]):
        self.nodes = {node.id: node for node in nodes}
        self.data_items = {}  # 存储数据项信息
        self.replica_strategy = ReplicaStrategy()  # 复用之前的策略类
    
    def add_data_item(self, data_info: DataInfo):
        """添加数据项"""
        self.data_items[data_info.id] = data_info
    
    def get_node_load_score(self, node_id: str) -> float:
        """计算节点负载评分（越低越好）"""
        node = self.nodes.get(node_id)
        if not node:
            return float('inf')
        
        # 综合考虑负载和容量使用率
        load_score = node.load / 100  # 负载占比
        capacity_usage = (node.capacity - node.available_capacity) / node.capacity  # 容量使用率
        
        return load_score * 0.6 + capacity_usage * 0.4
    
    def get_rack_distribution(self, replica_nodes: List[str]) -> Dict[str, int]:
        """获取副本的机架分布"""
        rack_count = {}
        for node_id in replica_nodes:
            node = self.nodes.get(node_id)
            if node:
                rack = node.rack
                rack_count[rack] = rack_count.get(rack, 0) + 1
        return rack_count
    
    def schedule_replica_actions(self) -> List[Tuple[ReplicaAction, str, str, str]]:
        """调度副本操作"""
        actions = []
        
        for data_id, data_info in self.data_items.items():
            # 计算应有的副本数
            target_replicas = self.replica_strategy.get_replica_count(data_info.importance)
            target_count = target_replicas["final_count"]
            
            current_replicas = data_info.replicas.copy()
            current_count = len(current_replicas)
            
            # 检查节点状态，移除失效节点
            valid_replicas = [r for r in current_replicas if self.nodes.get(r) and self.nodes[r].status == "healthy"]
            
            if len(valid_replicas) < current_count:
                # 有节点失效，需要重新调度
                current_replicas = valid_replicas
                current_count = len(current_replicas)
            
            # 副本数不足，需要创建
            if current_count < target_count:
                missing_count = target_count - current_count
                new_replicas = self._select_nodes_for_replicas(missing_count, current_replicas, data_info.size)
                for node_id in new_replicas:
                    actions.append((ReplicaAction.CREATE, data_id, node_id, ""))
                    current_replicas.append(node_id)
            
            # 副本数过多，需要删除
            elif current_count > target_count:
                excess_count = current_count - target_count
                nodes_to_remove = self._select_replicas_to_remove(excess_count, current_replicas, data_info)
                for node_id in nodes_to_remove:
                    actions.append((ReplicaAction.DELETE, data_id, node_id, ""))
                    current_replicas.remove(node_id)
            
            # 检查机架分布是否合理
            rack_distribution = self.get_rack_distribution(current_replicas)
            if len(rack_distribution) < min(2, len(set(node.rack for node in self.nodes.values()))):
                # 机架分布不合理，需要迁移
                migration_actions = self._optimize_rack_distribution(data_info, current_replicas)
                actions.extend(migration_actions)
            
            # 更新数据项的副本信息
            data_info.replicas = current_replicas
        
        return actions
    
    def _select_nodes_for_replicas(self, count: int, existing_replicas: List[str], data_size: int) -> List[str]:
        """为数据选择新的副本节点"""
        # 排除已有的副本节点和不健康的节点
        available_nodes = [
            node_id for node_id, node in self.nodes.items()
            if node_id not in existing_replicas and node.status == "healthy" and node.available_capacity >= data_size
        ]
        
        if len(available_nodes) < count:
            # 可用节点不足，返回所有可用节点
            return available_nodes
        
        # 按负载评分排序，选择负载最低的节点
        sorted_nodes = sorted(available_nodes, key=self.get_node_load_score)
        return sorted_nodes[:count]
    
    def _select_replicas_to_remove(self, count: int, replicas: List[str], data_info: DataInfo) -> List[str]:
        """选择要删除的副本"""
        # 优先删除负载高的节点上的副本
        sorted_replicas = sorted(replicas, key=self.get_node_load_score, reverse=True)
        return sorted_replicas[:count]
    
    def _optimize_rack_distribution(self, data_info: DataInfo, current_replicas: List[str]) -> List[Tuple[ReplicaAction, str, str, str]]:
        """优化机架分布"""
        actions = []
        rack_distribution = self.get_rack_distribution(current_replicas)
        
        # 找到副本数过多的机架
        max_rack = max(rack_distribution.keys(), key=lambda x: rack_distribution[x])
        max_count = rack_distribution[max_rack]
        
        # 找到副本数过少或没有副本的机架
        all_racks = set(node.rack for node in self.nodes.values() if node.status == "healthy")
        under_represented_racks = [rack for rack in all_racks if rack_distribution.get(rack, 0) < 1]
        
        if under_represented_racks and max_count > 1:
            # 从副本过多的机架迁移一个副本到副本过少的机架
            source_node = next((node_id for node_id in current_replicas 
                              if self.nodes.get(node_id) and self.nodes[node_id].rack == max_rack), None)
            
            if source_node:
                # 选择目标机架
                target_rack = under_represented_racks[0]
                # 在目标机架中选择一个合适的节点
                target_nodes = [node_id for node_id, node in self.nodes.items()
                              if node.rack == target_rack and node.status == "healthy" 
                              and node.available_capacity >= data_info.size
                              and node_id not in current_replicas]
                
                if target_nodes:
                    target_node = min(target_nodes, key=self.get_node_load_score)
                    actions.append((ReplicaAction.MIGRATE, data_info.id, target_node, source_node))
        
        return actions

# 使用示例
# 创建节点信息
nodes = [
    NodeInfo("node1", "rack1", 1000000, 800000, 30.0, "healthy"),
    NodeInfo("node2", "rack1", 1000000, 700000, 50.0, "healthy"),
    NodeInfo("node3", "rack2", 1000000, 900000, 20.0, "healthy"),
    NodeInfo("node4", "rack2", 1000000, 600000, 40.0, "degraded"),
    NodeInfo("node5", "rack3", 1000000, 850000, 25.0, "healthy"),
]

# 创建调度器
scheduler = ReplicaScheduler(nodes)

# 添加数据项
data_items = [
    DataInfo("data1", 100000, "critical", 0.8, ["node1", "node3", "node5"]),
    DataInfo("data2", 200000, "high", 0.5, ["node1", "node2"]),
    DataInfo("data3", 50000, "medium", 0.2, ["node4", "node5"]),
]

for data_item in data_items:
    scheduler.add_data_item(data_item)

# 执行调度
actions = scheduler.schedule_replica_actions()

print("=== 副本调度操作 ===")
for action, data_id, target_node, source_node in actions:
    if action == ReplicaAction.CREATE:
        print(f"创建副本: 数据 {data_id} -> 节点 {target_node}")
    elif action == ReplicaAction.DELETE:
        print(f"删除副本: 数据 {data_id} <- 节点 {target_node}")
    elif action == ReplicaAction.MIGRATE:
        print(f"迁移副本: 数据 {data_id} 从节点 {source_node} -> 节点 {target_node}")
```

### 6.1.2.4.2 副本生命周期管理

```python
# 副本生命周期管理示例
import time
import threading
from typing import Dict, List, Callable
from dataclasses import dataclass
from enum import Enum

class ReplicaState(Enum):
    CREATING = "creating"
    HEALTHY = "healthy"
    DEGRADED = "degraded"
    FAILED = "failed"
    DELETING = "deleting"

@dataclass
class ReplicaInfo:
    id: str
    data_id: str
    node_id: str
    state: ReplicaState
    created_time: float
    last_heartbeat: float
    size: int
    hash: str

class ReplicaLifecycleManager:
    def __init__(self):
        self.replicas = {}  # replica_id -> ReplicaInfo
        self.data_replicas = {}  # data_id -> [replica_id, ...]
        self.node_replicas = {}  # node_id -> [replica_id, ...]
        self.state_callbacks = {}  # state -> [callback, ...]
        self.lock = threading.RLock()
    
    def register_state_callback(self, state: ReplicaState, callback: Callable):
        """注册状态变更回调"""
        if state not in self.state_callbacks:
            self.state_callbacks[state] = []
        self.state_callbacks[state].append(callback)
    
    def create_replica(self, replica_id: str, data_id: str, node_id: str, size: int, data_hash: str) -> bool:
        """创建副本"""
        with self.lock:
            if replica_id in self.replicas:
                return False
            
            replica_info = ReplicaInfo(
                id=replica_id,
                data_id=data_id,
                node_id=node_id,
                state=ReplicaState.CREATING,
                created_time=time.time(),
                last_heartbeat=time.time(),
                size=size,
                hash=data_hash
            )
            
            self.replicas[replica_id] = replica_info
            
            # 更新数据到副本的映射
            if data_id not in self.data_replicas:
                self.data_replicas[data_id] = []
            self.data_replicas[data_id].append(replica_id)
            
            # 更新节点到副本的映射
            if node_id not in self.node_replicas:
                self.node_replicas[node_id] = []
            self.node_replicas[node_id].append(replica_id)
            
            # 触发状态变更回调
            self._trigger_callbacks(replica_id, ReplicaState.CREATING)
            
            return True
    
    def update_replica_state(self, replica_id: str, new_state: ReplicaState) -> bool:
        """更新副本状态"""
        with self.lock:
            if replica_id not in self.replicas:
                return False
            
            old_state = self.replicas[replica_id].state
            self.replicas[replica_id].state = new_state
            self.replicas[replica_id].last_heartbeat = time.time()
            
            # 触发状态变更回调
            self._trigger_callbacks(replica_id, new_state, old_state)
            
            return True
    
    def update_replica_heartbeat(self, replica_id: str) -> bool:
        """更新副本心跳"""
        with self.lock:
            if replica_id not in self.replicas:
                return False
            
            self.replicas[replica_id].last_heartbeat = time.time()
            return True
    
    def delete_replica(self, replica_id: str) -> bool:
        """删除副本"""
        with self.lock:
            if replica_id not in self.replicas:
                return False
            
            replica_info = self.replicas[replica_id]
            
            # 更新状态为删除中
            old_state = replica_info.state
            replica_info.state = ReplicaState.DELETING
            
            # 从数据映射中移除
            if replica_info.data_id in self.data_replicas:
                if replica_id in self.data_replicas[replica_info.data_id]:
                    self.data_replicas[replica_info.data_id].remove(replica_id)
            
            # 从节点映射中移除
            if replica_info.node_id in self.node_replicas:
                if replica_id in self.node_replicas[replica_info.node_id]:
                    self.node_replicas[replica_info.node_id].remove(replica_id)
            
            # 删除副本信息
            del self.replicas[replica_id]
            
            # 触发状态变更回调
            self._trigger_callbacks(replica_id, ReplicaState.DELETING, old_state)
            
            return True
    
    def get_replica_info(self, replica_id: str) -> ReplicaInfo:
        """获取副本信息"""
        with self.lock:
            return self.replicas.get(replica_id)
    
    def get_replicas_by_data(self, data_id: str) -> List[ReplicaInfo]:
        """根据数据ID获取副本列表"""
        with self.lock:
            replica_ids = self.data_replicas.get(data_id, [])
            return [self.replicas[rid] for rid in replica_ids if rid in self.replicas]
    
    def get_replicas_by_node(self, node_id: str) -> List[ReplicaInfo]:
        """根据节点ID获取副本列表"""
        with self.lock:
            replica_ids = self.node_replicas.get(node_id, [])
            return [self.replicas[rid] for rid in replica_ids if rid in self.replicas]
    
    def get_replicas_by_state(self, state: ReplicaState) -> List[ReplicaInfo]:
        """根据状态获取副本列表"""
        with self.lock:
            return [replica for replica in self.replicas.values() if replica.state == state]
    
    def check_heartbeat_timeout(self, timeout: float = 30.0) -> List[str]:
        """检查心跳超时的副本"""
        with self.lock:
            current_time = time.time()
            timeout_replicas = []
            
            for replica_id, replica_info in self.replicas.items():
                if current_time - replica_info.last_heartbeat > timeout:
                    timeout_replicas.append(replica_id)
            
            return timeout_replicas
    
    def _trigger_callbacks(self, replica_id: str, new_state: ReplicaState, old_state: ReplicaState = None):
        """触发状态变更回调"""
        if new_state in self.state_callbacks:
            replica_info = self.replicas.get(replica_id)
            if replica_info:
                for callback in self.state_callbacks[new_state]:
                    try:
                        callback(replica_info, old_state)
                    except Exception as e:
                        print(f"Callback error: {e}")

# 使用示例
lifecycle_manager = ReplicaLifecycleManager()

# 注册状态变更回调
def on_replica_healthy(replica_info, old_state):
    print(f"副本 {replica_info.id} 变为健康状态，数据: {replica_info.data_id}")

def on_replica_failed(replica_info, old_state):
    print(f"副本 {replica_info.id} 失败，数据: {replica_info.data_id}")

lifecycle_manager.register_state_callback(ReplicaState.HEALTHY, on_replica_healthy)
lifecycle_manager.register_state_callback(ReplicaState.FAILED, on_replica_failed)

# 创建副本
print("=== 创建副本 ===")
lifecycle_manager.create_replica("replica_001", "data_001", "node1", 102400, "hash123")
lifecycle_manager.create_replica("replica_002", "data_001", "node2", 102400, "hash123")
lifecycle_manager.create_replica("replica_003", "data_002", "node3", 204800, "hash456")

# 更新副本状态
print("\n=== 更新副本状态 ===")
lifecycle_manager.update_replica_state("replica_001", ReplicaState.HEALTHY)
lifecycle_manager.update_replica_state("replica_002", ReplicaState.HEALTHY)
lifecycle_manager.update_replica_state("replica_003", ReplicaState.HEALTHY)

# 更新心跳
print("\n=== 更新心跳 ===")
lifecycle_manager.update_replica_heartbeat("replica_001")
lifecycle_manager.update_replica_heartbeat("replica_002")

# 查询副本信息
print("\n=== 查询副本信息 ===")
replica_info = lifecycle_manager.get_replica_info("replica_001")
if replica_info:
    print(f"副本ID: {replica_info.id}")
    print(f"数据ID: {replica_info.data_id}")
    print(f"节点ID: {replica_info.node_id}")
    print(f"状态: {replica_info.state}")
    print(f"创建时间: {replica_info.created_time}")

# 根据数据ID查询副本
print("\n=== 根据数据ID查询副本 ===")
data_replicas = lifecycle_manager.get_replicas_by_data("data_001")
for replica in data_replicas:
    print(f"  副本ID: {replica.id}, 节点: {replica.node_id}, 状态: {replica.state}")

# 根据节点ID查询副本
print("\n=== 根据节点ID查询副本 ===")
node_replicas = lifecycle_manager.get_replicas_by_node("node1")
for replica in node_replicas:
    print(f"  副本ID: {replica.id}, 数据: {replica.data_id}, 状态: {replica.state}")

# 检查心跳超时
print("\n=== 检查心跳超时 ===")
# 模拟一个副本心跳超时
lifecycle_manager.replicas["replica_003"].last_heartbeat = time.time() - 60  # 60秒前的心跳
timeout_replicas = lifecycle_manager.check_heartbeat_timeout(30.0)
print(f"心跳超时的副本: {timeout_replicas}")

# 删除副本
print("\n=== 删除副本 ===")
deleted = lifecycle_manager.delete_replica("replica_003")
print(f"删除副本结果: {deleted}")

# 再次查询数据副本
data_replicas_after_delete = lifecycle_manager.get_replicas_by_data("data_002")
print(f"删除后数据 data_002 的副本数: {len(data_replicas_after_delete)}")
```

## 总结

多副本机制是分布式文件存储系统中保障数据可靠性和高可用性的核心技术。通过在多个节点上存储数据副本，系统可以在部分节点发生故障时仍然保证数据的完整性和可访问性。

本章详细介绍了多副本机制的实现原理和关键技术：

1. **副本数量策略**：根据数据重要性、机架感知和可用性目标综合确定副本数量
2. **副本放置策略**：通过机架感知、容量感知和负载均衡等策略优化副本分布
3. **多副本同步机制**：支持同步、异步和法定数量等多种同步模式
4. **副本一致性维护**：通过元数据管理和后台检查确保副本数据一致性
5. **副本调度与管理**：实现副本的动态创建、删除和迁移
6. **副本生命周期管理**：完整的副本状态管理和监控机制

在实际应用中，需要根据具体的业务需求和系统约束选择合适的副本策略，并通过合理的实现确保系统的高性能、高可靠性和高可用性。同时，还需要考虑存储成本、网络开销和系统复杂度等因素，找到最佳的平衡点。